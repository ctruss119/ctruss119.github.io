---
title:  "10월/ 3. BERT 공부하기(1)- BERT: Pre-training of Deep Bidirectional Transformers for
 Language Understanding 논문 리뷰 "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section0">0. Abstract. </a></li>
  <li><a href="#section1">1. Introduction. </a></li>
  <li><a href="#section2">2. Related Work </a></li>
  <li><a href="#section2.1">2.1 Unsupervised Feature-based Approaches </a></li>
</ul>

---
 [논문](https://arxiv.org/pdf/1810.04805) 

# 0. Abstract. <a name="section0"></a>
우리는 BERT(Bidirectional Encoder Representations from Transformers)라는 새로운 언어 표현 모델을 소개한다. 최근의 언어 표현 모델(Peters et al., 2018a; Radford et al., 2018)과는 달리, BERT는 모든 층에서 좌측과 우측 문맥을 공동으로 조건화하여 비지도 학습된 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계되었다. 그 결과, 사전 훈련된 BERT 모델은 추가적인 출력 층 하나만으로도 질문 응답이나 언어 추론과 같은 다양한 작업에서 최첨단 모델을 만들 수 있으며, 작업에 특화된 아키텍처의 수정이 거의 필요하지 않다.

BERT는 개념적으로 간단하면서도 경험적으로 강력하다. BERT는 열한 가지 자연어 처리 작업에서 새로운 최첨단 결과를 얻었으며, GLUE 점수를 80.5%로 끌어올렸고(절대 7.7% 포인트 향상), MultiNLI 정확도를 86.7%로(절대 4.6% 향상), SQuAD v1.1 질문 응답 테스트 F1 점수를 93.2로(절대 1.5% 포인트 향상), 그리고 SQuAD v2.0 테스트 F1 점수를 83.1로(절대 5.1% 포인트 향상) 향상시켰다.


# 1. Introduction <a name="section1"></a>
언어 모델 사전 훈련이 많은 자연어 처리 작업의 성능을 향상시키는 데 효과적이라는 것이 입증되었다(Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). 이에는 자연어 추론(Bowman et al., 2015; Williams et al., 2018)이나 패러프레이징(Dolan and Brockett, 2005)과 같이 문장을 전체적으로 분석하여 문장 간의 관계를 예측하는 문장 수준 작업과, 명명된 개체 인식이나 질문 응답처럼 토큰 수준에서 세밀한 출력을 요구하는 토큰 수준 작업(Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016)이 포함된다.
사전 훈련된 언어 표현을 다운스트림 작업에 적용하는 두 가지 기존 전략이 있다: feature-based 접근법과 fine-tuning 접근법이다. ELMo(Peters et al., 2018a)와 같은 feature-based 접근법은 사전 훈련된 표현을 추가적인 특징으로 포함하는 작업별 아키텍처를 사용한다. OpenAI GPT(Radford et al., 2018)와 같은 fine-tuning 접근법은 최소한의 작업별 매개변수를 도입하고, 단순히 모든 사전 훈련된 매개변수를 미세 조정하여 다운스트림 작업에서 학습한다. 두 접근법은 사전 훈련 중 동일한 목표 함수를 공유하며, 단방향 언어 모델을 사용하여 일반적인 언어 표현을 학습한다.

우리는 현재 기술이 특히 fine-tuning 접근법에서 사전 훈련된 표현의 잠재력을 제한한다고 주장한다. 주요 제한은 표준 언어 모델이 단방향이라는 점으로, 이는 사전 훈련 중에 사용할 수 있는 아키텍처 선택을 제한한다. 예를 들어, OpenAI GPT에서 저자들은 좌에서 우로의 아키텍처를 사용하며, Transformer(Vaswani et al., 2017)의 자기-주의 레이어에서 각 토큰은 이전 토큰들만 참조할 수 있다. 이러한 제한은 문장 수준 작업에서 최적이 아니며, 양방향 문맥을 포함하는 것이 중요한 질문 응답과 같은 토큰 수준 작업에서 fine-tuning 기반 접근법을 적용할 때 매우 해로울 수 있다.

이 논문에서 우리는 BERT(Bidirectional Encoder Representations from Transformers)를 제안하여 fine-tuning 기반 접근법을 개선한다. BERT는 masked language model(MLM) 사전 훈련 목표를 사용하여 앞서 언급한 단방향성 제약을 해소하며, 이는 Cloze task(Taylor, 1953)에서 영감을 받았다. Masked language model은 입력에서 일부 토큰을 무작위로 마스킹하고, 그 문맥을 기반으로 마스킹된 단어의 원래 어휘 ID를 예측하는 것이다. 좌에서 우로의 언어 모델 사전 훈련과 달리, MLM 목표는 좌우 문맥을 융합할 수 있는 표현을 가능하게 하며, 이를 통해 깊이 있는 양방향 Transformer를 사전 훈련할 수 있다. 또한, 마스킹된 언어 모델 외에도 "다음 문장 예측" 작업을 사용하여 텍스트 쌍의 표현을 함께 사전 훈련한다.

이 논문의 기여는 다음과 같다:

우리는 언어 표현을 위한 양방향 사전 훈련의 중요성을 입증한다. **Radford et al. (2018)**과는 달리, BERT는 깊이 있는 양방향 표현을 사전 훈련할 수 있게 하는 마스킹된 언어 모델을 사용한다. 이는 **Peters et al. (2018a)**가 독립적으로 훈련된 좌에서 우, 우에서 좌 LMs를 얕게 연결한 것과도 대조된다.
우리는 사전 훈련된 표현이 많은 작업에 특화된 아키텍처의 필요성을 줄여준다는 것을 보여준다. BERT는 문장 수준과 토큰 수준 작업 모두에서 최첨단 성능을 달성한 첫 번째 fine-tuning 기반 표현 모델로, 많은 작업별 아키텍처를 능가한다.
BERT는 11가지 NLP 작업에서 최첨단 성능을 크게 향상시켰다. 코드는 다음에서 제공된다: https://github.com/google-research/bert.

# 2.  Related Work <a name="section2"></a>
일반 언어 표현을 사전 훈련하는 역사는 길며, 이 섹션에서는 가장 널리 사용되는 접근법들을 간략히 검토한다.

# 2.1  Unsupervised Feature-based Approaches <a name="section2.1"></a>
# 3.  <a name="section2"></a>
# 4.  <a name="section2"></a>
# 5.  <a name="section2"></a>
# 6.  <a name="section2"></a>
# 7.  <a name="section2"></a>
# 8.  <a name="section2"></a>
# 9.  <a name="section2"></a>


