---
title:  "10월/ 2.  Attention Is All You Need 논문 읽기  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section0">0. Abstract.  </a></li>   
  <li><a href="#section1">1. Introduction  </a></li>   
  <li><a href="#section2">2. Background  </a></li>
  <li><a href="#section3">3. ModelArchitecture </a></li>
  <li><a href="#section4">4.  WhySelf-Attention </a></li>
  <li><a href="#section5">5.  Training </a></li>
  <li><a href="#section6">6.  Results </a></li>
  <li><a href="#section7">7.  Conclusion </a></li>
  <li><a href="#section8">8.  정리 </a></li>  
</ul>

-------------------------------------------------------------------   

# 0. 
가능한 원문 그대로 읽어서 기록하고자 한다.   
그러다가 모르는 개념이 나오면 중간중간 설명하면서 작성하려고 한다.   
다른 블로그도 참고해가면서 읽으려고 한다.

# <a id="section0"></a>0.  Abstract. 

  현재 많이 쓰이는 시퀀스 변환 모델은 복잡한 순환 신경망(Recurrent Neural Networks, RNN) 또는 합성곱 신경망(Convolutional Neural Networks, CNN)을 기반으로 하며,  인코더와 디코더를 포함하고 있다. 가장 성능이 좋은 모델들은 인코더와 디코더를 어텐션 메커니즘을 통해 연결한다. 

* 시퀀스 변환 모델: 입력 시퀀스를 받아서 출력 시퀀스로 변환하는 모델. 예를 들어, 기계 번역에서 입력된 문장을 다른 언어로 변환하는 작업이 이에 해당한다. 자연어 처리(NLP)에서 자주 사용되며, 번역뿐 아니라 음성 인식, 요약, 텍스트 생성 같은 작업에 활용된다.   

* 순환 신경망(RNN): **순차적 데이터(시퀀스)**를 처리하기 위해 설계된 신경망. 시퀀스의 각 단계에서 이전 단계의 출력을 다음 단계로 전달해, 이전 정보를 기억하고 현재 입력을 처리할 수 있다. 주로 자연어 처리에서 사용되며, 텍스트 생성, 번역, 음성 인식 등에 활용된다. 시퀀스가 길어지면 정보가 소실되거나 장기 의존 관계(long-term dependencies)를 학습하기 어려운 단점이 있다.   

* 합성곱 신경망(CNN): **공간적 데이터(예: 이미지)**를 처리하는 데 주로 사용되며, 특징 추출에 탁월한 성능을 보입니다. 입력 데이터를 여러 필터를 통해 합성곱(Convolution) 연산을 적용하여 중요한 특징을 추출하고, 이를 통해 다양한 작업을 수행합니다. 최근에는 텍스트 처리 작업에도 CNN이 활용되고 있습니다.
특징: 주로 이미지 인식 및 영상 처리에 사용되지만, NLP에서도 합성곱을 통해 문장의 패턴을 추출하는 방식으로 응용됩니다.   

* 인코더: 입력 데이터를 **압축된 표현(벡터)**으로 변환하는 네트워크입니다. NLP 작업에서 인코더는 입력 시퀀스를 처리하여, 그 의미를 함축한 벡터 표현을 생성합니다. 예를 들어, 문장을 하나의 고차원 벡터로 변환하는 역할을 합니다.   
 
* 디코더: 인코더에서 생성된 압축된 벡터 표현을 입력받아, 이를 바탕으로 출력 시퀀스를 생성하는 역할을 합니다. 주로 번역 작업에서, 인코더가 입력된 문장을 벡터로 변환한 후, 디코더가 해당 벡터를 사용하여 목표 언어로 변환된 문장을 출력합니다.   
 
 * 어텐션 매커니즘: 입력 시퀀스의 각 요소를 처리할 때 다른 중요한 요소에 더 많은 가중치를 부여하여 처리하는 기법입니다. 특히, 문장 내에서 중요한 단어에 집중하여 그 관계를 더 잘 파악할 수 있도록 도와줍니다.
사용 이유: RNN 등의 모델은 시퀀스가 길어지면 정보를 잊는 문제가 있는데, 어텐션 메커니즘은 모든 입력 요소를 한 번에 살펴보며 중요한 정보를 놓치지 않게 해줍니다.

이 논문에서는 이러한 복잡성을 없애고 순수하게 어텐션 메커니즘만을 기반으로 한 새로운 간단한 네트워크 아키텍처인 **트랜스포머(Transformer)**를 제안한다. 이 모델은 순환이나 합성곱을 완전히 배제한다. 두 가지 기계 번역 작업에 대한 실험 결과, 이 모델(트랜스포머)이 기존의 모델들보다 더 우수한 품질을 보이며, 병렬화 가능성이 더 높고 훈련 시간이 현저히 적게 소요된다는 것을 확인했다. 

* 기계 번역 작업: 한 언어의 텍스트를 다른 언어로 자동으로 변환하는 작업. NLP의 대표적인 응용 분야로, 딥러닝과 트랜스포머 모델은 기계 번역의 성능을 크게 향상시켰다. 예시로, 영어에서 독일어로 문장을 번역하는 작업이 있다.   
   
* 병렬화 가능성: 모델이 여러 작업을 동시에 처리할 수 있는 능력. RNN은 순차적으로 데이터를 처리하기 때문에 병렬화가 어렵지만, 트랜스포머는 병렬로 데이터를 처리할 수 있어, 더 빠른 훈련 속도를 가잔다. 병렬화가 잘 이루어지면 훈련 속도가 크게 향상되며, 대규모 데이터를 더 효율적으로 처리할 수 있습니다.   

우리의 모델은 2014년 WMT 영어-독일어 번역 작업에서 28.4 BLEU 점수를 기록했으며, 이는 기존 최고 성능을 보였던 모델들(앙상블 포함)보다 2 BLEU 이상 개선된 결과이다. 또한, 2014년 WMT 영어-프랑스어 번역 작업에서는 41.8 BLEU로 단일 모델 기준 새로운 최고 성능을 기록했으며, 8개의 GPU에서 3.5일 동안 훈련한 결과, 기존 문헌에 나와 있는 최고 성능 모델들보다 훈련 비용이 매우 적었다. 우리는 트랜스포머가 영어 구문 분석(constituency parsing)에도 잘 적용된다는 것을 확인했으며, 대규모 데이터셋과 제한된 데이터셋 모두에서 성공적인 결과를 보였다. 

* BLEU: BLEU는 기계 번역의 품질을 평가하기 위해 사용되는 지표입니다. 번역된 문장이 정답 문장과 얼마나 유사한지를 평가하는데, 주로 단어 간의 유사성과 정확도를 측정합니다.
특징: BLEU 점수는 0에서 100까지의 범위를 가지며, 점수가 높을수록 번역된 문장이 원문에 가깝다는 것을 의미합니다.


# <a id="section1"></a>1.  Introduction

순환 신경망(Recurrent Neural Networks, RNN), 장단기 기억(Long Short-Term Memory, LSTM), 게이트 순환 신경망(Gated Recurrent Neural Networks, GRU)은 언어 모델링 및 기계 번역과 같은 sequence modeling 및 transduction 문제애서 sota로 자리를 잡았다. 이후 많은 연구들이 순환 언어 모델과 인코더-디코더 아키텍처의 경계를 확장하기 위해 지속적으로 노력해 왔다.   

순환 모델(Recurrent model)은 일반적으로 input 및 output sequence의 각 위치에 따라 계산을 분리하여 처리한다. 시퀀스의 각 위치(symbol positions)를 steps와 위치에 따라 정렬하여, 이전 은닉 상태(hidden state )\( h_{t-1}\)와 position t가 input인 hidden state \( h_t \)를 생성한다.    
* 순차적인 계산에 대해 풀어서 설명하자면: 순환 신경망(RNN)과 같은 모델은 입력 시퀀스의 각 단어(또는 토큰)를 시간 순서대로 처리한다. 즉, 시퀀스의 각 위치(예: t번째 단어)를 차례로 처리하며, 이전 시점의 계산 결과(은닉 상태)를 기억한 상태에서 현재 시점의 입력을 반영해 계산을 수행함. 즉슨 시퀀스의 t번째 위치에서 출력이 결정될 때, t-1번째 위치의 정보(은닉 상태)와 현재 t번째 입력이 모두 반영됨.

이러한 순차적 처리 특성(ht-1 처리 후에야 ht를 처리할 수 있는 순차적인 특성)은 병렬 처리를 어렵게 만드는데, 이는 시퀀스 길이가 길어질수록 메모리 제한으로 인해 여러 예제를 배치(batch)로 처리하는 데 한계를 겪게 된다. 최근 연구는 계산 효율성을 개선하기 위해 인수분해 기법과 조건부 계산을 사용하여 큰 발전을 이루었으며, 후자는 모델 성능도 향상시켰다. 그러나 순차적 계산의 근본적인 제약은 여전히 남아 있다.   

* 병렬화가 어려워지는 이유: 이 방식은 이전 시점의 계산이 끝나야 다음 시점의 계산을 할 수 있기 때문에 병렬로 처리하기 어렵다는 단점이 있습니다. 즉, 시퀀스의 각 요소가 독립적으로 동시에 처리되지 않고, 하나씩 차례로 계산되어야 한다는 것을 의미함. 요약하자면 각 시퀀스 위치에서의 계산이 순차적으로 일어나며 이전 결과에 의존한다는 단점이 있다는 것이다. 

어텐션 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링과 변환 모델의 필수적인 부분이 되었다. 이는 입력 또는 출력 시퀀스에서 거리와 상관없이 의존성을 모델링할 수 있게 해준다. 하지만 일부 경우를 제외하고, 이러한 어텐션 메커니즘은 대부분 순환 신경망과 함께 사용된다. 
* 효율적인 병렬화가 불가능하다는 것을 의미함 

본 연구에서는 순환 구조(recurrence)를 제거하고 전적으로 어텐션 메커니즘에 의존하여, 입력과 출력 간의 전역 의존성을 모델링하는 트랜스포머(Transformer)를 제안한다. 

트랜스포머는 병렬화 가능성을 크게 향상시켰으며, 8개의 P100 GPU에서 12시간 만에 훈련하여 번역 품질에서 새로운 최고 성능에 도달할 수 있었다.

* cpu와 gpu로 본 병렬화 차이를 시각적으로 이해시켜주는 [영상](https://www.youtube.com/watch?v=1BAZf3PsjWA0)

# <a id="section2"></a>2. Background

순차적인 계산을 줄이려는 모델(Extended Neural GPU, ByteNet, ConvS2S 등)이 있었다. 이들 모두 합성곱 신경망 CNN(Convolutional Neural Networks)을 기본 구성 요소로 사용하여, 입력 및 출력의 모든 위치에 대해 병렬로  hidden representations를 계산한다. 이러한 모델들에서, 임의의 두 입력 또는 출력 위치 간의 신호를 연결하는 데 필요한 연산 수는 그 위치 간의 거리에 따라 증가한다. ConvS2S의 경우에는 선형적으로, ByteNet은 로그 형태로 증가한다. 이는 멀리 떨어진 위치 간의 의존성을 학습하는 것을 더 어렵게 만든다.   

트랜스포머에서는 이를 상수 개수의 연산으로 줄였지만, 어텐션 가중치가 적용된 위치의 평균으로 인해 해상도가 감소하는 문제가 발생할 수 있다. 이 문제는 3.2절에서 설명하는 멀티-헤드 어텐션(Multi-Head Attention)으로 보완된다.   
Self-attention(intra-attention)은 단일 시퀀스의 다양한 위치 간의 관계를 계산하여 그 시퀀스의 표현을 생성하는 어텐션 메커니즘입니다. Self-attention은 독해(Reading Comprehension), 요약(Abstractive Summarization), 텍스트 함의(Textual Entailment), 작업 독립적 문장 표현 학습 등 다양한 작업에서 성공적으로 사용되었다.

End-to-end 메모리 네트워크는 sequence aligned recurrence(시퀀스 정렬 순환) 대신에 순환 어텐션 메커니즘(recurrent attention mechanism)을 기반으로 하며, 간단한 언어 질문 응답(single-language QA) 및 언어 모델링 작업에서 좋은 성능을 보인다.

한편, 트랜스포머는 RNN이나 CNN을 사용하지 않고 완전히 Self-attention만을 사용하여 입력과 출력의 표현을 계산하는 최초의 변환(transduction) 모델이다. 

다음 섹션에서는 트랜스포머에 대해 설명하고, Self-attention을 어떻게 동작하게 하고 기존의 모델들보다 가지는 장점에 대해 논의할 것이다.   

# <a id="section3"></a>3.  ModelArchitecture

##  3.1 Encoder and Decoder Stacks
### Encorder
### Decorder

##  3.2 Attention
### 3.2.1 Scaled Dot-Product Attention
### 3.2.2 Multi-Head Attention
### 3.2.3 Applications of Attention in our Model

##  3.3 Position-wise Feed-Forward Networks

##  3.4 Embeddings and Softmax

##  3.5 Positional Encoding

# <a id="section4"></a>4.  WhySelf-Attention
# <a id="section5"></a>5.  Training
## 5.1 Training Data and Batching
## 5.2 Hardware and Schedule
## 5.3 Optimizer
## 5.4 Regularization

# <a id="section6"></a>6.  Results
##  6.1 MachineTranslation
##  6.2 ModelVariations
##  6.3 EnglishConstituencyParsing

# <a id="section7"></a>7.  Conclusion

# <a id="section8"></a>8.  정리 



















2. 트랜스포머란 무엇인가?
트랜스포머의 기본 개념
순차적 처리 vs 병렬 처리
트랜스포머의 주요 특징 (Attention 기반 처리)

3. 어텐션 메커니즘의 개요
어텐션(Attention)이란?
기존 모델에서의 어텐션 사용 방식
논문에서 제안된 Self-Attention의 역할

4. 트랜스포머의 구조
트랜스포머의 인코더-디코더 구조
인코더와 디코더의 역할
트랜스포머 모델의 각 구성 요소 설명
Input Embedding과 Positional Encoding
Multi-Head Self-Attention
Feed-Forward Networks
Layer Normalization과 Residual Connection

5. 어텐션 메커니즘 상세 설명
Self-Attention의 작동 원리
Query, Key, Value 개념
어텐션 가중치 계산 방법
Scaled Dot-Product Attention의 수식적 설명
Multi-Head Attention의 역할 및 효과

6. 트랜스포머의 인코더와 디코더
인코더(Encoder) 구조 분석
인코더의 층별 동작 방식
각 층에서의 Attention과 Feed-Forward 처리
디코더(Decoder) 구조 분석
디코더의 추가적인 Attention: Masked Self-Attention
Cross-Attention의 역할

7. 트랜스포머의 학습 과정
학습 방식: Pre-training과 Fine-tuning
Position-wise Feed-Forward Networks와 모델의 학습
데이터 전처리 및 학습 전략

8. 실험 및 성능 평가
트랜스포머의 실험 결과
기계 번역(WMT) 작업에서의 성능
기존 RNN/LSTM 기반 모델과의 성능 비교

9. 트랜스포머의 영향과 후속 연구
BERT, GPT, T5 등 트랜스포머의 발전
NLP 및 다양한 영역에서 트랜스포머 기반 모델이 끼친 영향

10. 결론 및 요약
트랜스포머의 주요 기여 요약
어텐션 기반 모델이 NLP 작업에 미친 혁신적 변화
미래 전망 및 트랜스포머의 응용 가능성
