---
title:  "10월/ 1. 트랜스포머에 대해 공부하기(2) - 논문 외) "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">1. 인코딩 </a></li>   
  <li><a href="#section2">2. 디코딩 </a></li>   
  <li><a href="#section3">3.  </a></li>
  <li><a href="#section4">4.  </a></li>
  <li><a href="#section5">5.  </a></li>
</ul>
-------------------------------------------------------------------   

[이전 포스트](https://ctruss119.github.io/study/gyul6/)에서는 [논문](https://arxiv.org/pdf/1706.03762)에 집중해서 트랜스포머를 공부함.   

논문을 읽고 보니 논문에서는 자세히 설명되지 않은 내용에 대해 포스팅해야 할 필요성을 느낌.   
(1) 기본적인 내용(인코더, 디코더의 자세한 구조 등)   
(2) 어텐션에 대해 자세하게   
(2) 트랜스포머의 종류   
(3) 트랜스포머 기반 모델들   

세미나에서 배운 내용들이라, 그때 필기한 내용을 바탕으로 자세히 정리하는 방식으로 포스팅을 하려고 함.   


아직 포스팅 안함 
목차가 뒤죽박죽임 

# <a id="section1"></a>1. 
# <a id="section1"></a>1. 
# <a id="section1"></a>1. 
# <a id="section1"></a>1. 
# <a id="section1"></a>1. 
# <a id="section1"></a>1. 

## 2-1. 인코더 구조 
### Input Embedding
#### Tokenizer
#### Embedding
### Positional Encoding
### Multi-Head Attention
#### Self-Attention
### Attention 후 처리

## 2-2. 디코더 구조 
### Masked Self-Attention
### Cross-Attention
### 디코더의 후 처리


# <a id="section3"></a>3. 어텐션 

## 3-1. 어텐션 메커니즘의 개요
### 어텐션이란 무엇인가?
### 어텐션의 필요성

## 3-2. 어텐션의 작동 원리
### Query, Key, Value의 개념
### 어텐션 가중치 계산

## 3-3. Self-Attention
### Self-Attention의 역할과 장점
### 긴 문맥 처리

## 3-4. Multi-Head Attention
### 병렬적 주의 메커니즘
### 다양한 패턴 학습


