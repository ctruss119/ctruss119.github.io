---
title:  "10월/ 2.  Attention Is All You Need 논문 읽기  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">1.  </a></li>   
  <li><a href="#section2">2.   </a></li>   
  <li><a href="#section3">3.   </a></li>
  <li><a href="#section4">4.   </a></li>
</ul>
-------------------------------------------------------------------   

# 0. 
가능한 원문 그대로 읽어서 기록하고자 한다.   
그러다가 모르는 개념이 나오면 중간중간 설명하면서 작성하려고 한다.   
다른 블로그도 참고해가면서 읽으려고 한다.

# <a id="section1"></a>1.  Abstract. 

  현재 많이 쓰이는 시퀀스 변환 모델은 복잡한 순환 신경망(Recurrent Neural Networks, RNN) 또는 합성곱 신경망(Convolutional Neural Networks, CNN)을 기반으로 하며,  인코더와 디코더를 포함하고 있다.   

* 시퀀스 변환 모델: 입력 시퀀스를 받아서 출력 시퀀스로 변환하는 모델. 예를 들어, 기계 번역에서 입력된 문장을 다른 언어로 변환하는 작업이 이에 해당한다. 자연어 처리(NLP)에서 자주 사용되며, 번역뿐 아니라 음성 인식, 요약, 텍스트 생성 같은 작업에 활용된다.   

* 순환 신경망(RNN): **순차적 데이터(시퀀스)**를 처리하기 위해 설계된 신경망. 시퀀스의 각 단계에서 이전 단계의 출력을 다음 단계로 전달해, 이전 정보를 기억하고 현재 입력을 처리할 수 있다. 주로 자연어 처리에서 사용되며, 텍스트 생성, 번역, 음성 인식 등에 활용된다. 시퀀스가 길어지면 정보가 소실되거나 장기 의존 관계(long-term dependencies)를 학습하기 어려운 단점이 있다.   

* 합성곱 신경망(CNN): **공간적 데이터(예: 이미지)**를 처리하는 데 주로 사용되며, 특징 추출에 탁월한 성능을 보입니다. 입력 데이터를 여러 필터를 통해 합성곱(Convolution) 연산을 적용하여 중요한 특징을 추출하고, 이를 통해 다양한 작업을 수행합니다. 최근에는 텍스트 처리 작업에도 CNN이 활용되고 있습니다.
특징: 주로 이미지 인식 및 영상 처리에 사용되지만, NLP에서도 합성곱을 통해 문장의 패턴을 추출하는 방식으로 응용됩니다.   

* 인코더: 입력 데이터를 **압축된 표현(벡터)**으로 변환하는 네트워크입니다. NLP 작업에서 인코더는 입력 시퀀스를 처리하여, 그 의미를 함축한 벡터 표현을 생성합니다. 예를 들어, 문장을 하나의 고차원 벡터로 변환하는 역할을 합니다.   
 
* 디코더: 인코더에서 생성된 압축된 벡터 표현을 입력받아, 이를 바탕으로 출력 시퀀스를 생성하는 역할을 합니다. 주로 번역 작업에서, 인코더가 입력된 문장을 벡터로 변환한 후, 디코더가 해당 벡터를 사용하여 목표 언어로 변환된 문장을 출력합니다.   
 
 가장 성능이 좋은 모델들은 인코더와 디코더를 어텐션 메커니즘을 통해 연결한다. 
 * 어텐션 매커니즘: 입력 시퀀스의 각 요소를 처리할 때 다른 중요한 요소에 더 많은 가중치를 부여하여 처리하는 기법입니다. 특히, 문장 내에서 중요한 단어에 집중하여 그 관계를 더 잘 파악할 수 있도록 도와줍니다.
사용 이유: RNN 등의 모델은 시퀀스가 길어지면 정보를 잊는 문제가 있는데, 어텐션 메커니즘은 모든 입력 요소를 한 번에 살펴보며 중요한 정보를 놓치지 않게 해줍니다.

이 논문에서는 이러한 복잡성을 없애고 순수하게 어텐션 메커니즘만을 기반으로 한 새로운 간단한 네트워크 아키텍처인 **트랜스포머(Transformer)**를 제안한다. 이 모델은 순환이나 합성곱을 완전히 배제한다.

두 가지 기계 번역 작업에 대한 실험 결과, 이 모델(트랜스포머)이 기존의 모델들보다 더 우수한 품질을 보이며, 병렬화 가능성이 더 높고 훈련 시간이 현저히 적게 소요된다는 것을 확인했다. 
* 기계 번역 작업: 기계 번역은 한 언어의 텍스트를 다른 언어로 자동으로 변환하는 작업입니다. NLP의 대표적인 응용 분야로, 딥러닝과 트랜스포머 모델은 기계 번역의 성능을 크게 향상시켰습니다. 예시로, 영어에서 독일어로 문장을 번역하는 작업이 있습니다.   
   
* 병렬화 가능성: 병렬화 가능성은 모델이 여러 작업을 동시에 처리할 수 있는 능력을 의미합니다. RNN은 순차적으로 데이터를 처리하기 때문에 병렬화가 어렵지만, 트랜스포머는 병렬로 데이터를 처리할 수 있어, 더 빠른 훈련 속도를 가집니다.
장점: 병렬화가 잘 이루어지면 훈련 속도가 크게 향상되며, 대규모 데이터를 더 효율적으로 처리할 수 있습니다.   

우리의 모델은 2014년 WMT 영어-독일어 번역 작업에서 28.4 BLEU 점수를 기록했으며, 이는 기존 최고 성능을 보였던 모델들(앙상블 포함)보다 2 BLEU 이상 개선된 결과이다. 또한, 2014년 WMT 영어-프랑스어 번역 작업에서는 41.8 BLEU로 단일 모델 기준 새로운 최고 성능을 기록했으며, 8개의 GPU에서 3.5일 동안 훈련한 결과, 기존 문헌에 나와 있는 최고 성능 모델들보다 훈련 비용이 매우 적었다.
* BLEU: BLEU는 기계 번역의 품질을 평가하기 위해 사용되는 지표입니다. 번역된 문장이 정답 문장과 얼마나 유사한지를 평가하는데, 주로 단어 간의 유사성과 정확도를 측정합니다.
특징: BLEU 점수는 0에서 100까지의 범위를 가지며, 점수가 높을수록 번역된 문장이 원문에 가깝다는 것을 의미합니다.

우리는 트랜스포머가 영어 구문 분석(constituency parsing)에도 잘 적용된다는 것을 확인했으며, 대규모 데이터셋과 제한된 데이터셋 모두에서 성공적인 결과를 보였다. 



1. 서론
논문 소개 및 배경
기존 RNN, LSTM 모델의 한계
트랜스포머(Transformer)의 등장 배경

2. 트랜스포머란 무엇인가?
트랜스포머의 기본 개념
순차적 처리 vs 병렬 처리
트랜스포머의 주요 특징 (Attention 기반 처리)

3. 어텐션 메커니즘의 개요
어텐션(Attention)이란?
기존 모델에서의 어텐션 사용 방식
논문에서 제안된 Self-Attention의 역할

4. 트랜스포머의 구조
트랜스포머의 인코더-디코더 구조
인코더와 디코더의 역할
트랜스포머 모델의 각 구성 요소 설명
Input Embedding과 Positional Encoding
Multi-Head Self-Attention
Feed-Forward Networks
Layer Normalization과 Residual Connection

5. 어텐션 메커니즘 상세 설명
Self-Attention의 작동 원리
Query, Key, Value 개념
어텐션 가중치 계산 방법
Scaled Dot-Product Attention의 수식적 설명
Multi-Head Attention의 역할 및 효과

6. 트랜스포머의 인코더와 디코더
인코더(Encoder) 구조 분석
인코더의 층별 동작 방식
각 층에서의 Attention과 Feed-Forward 처리
디코더(Decoder) 구조 분석
디코더의 추가적인 Attention: Masked Self-Attention
Cross-Attention의 역할

7. 트랜스포머의 학습 과정
학습 방식: Pre-training과 Fine-tuning
Position-wise Feed-Forward Networks와 모델의 학습
데이터 전처리 및 학습 전략

8. 실험 및 성능 평가
트랜스포머의 실험 결과
기계 번역(WMT) 작업에서의 성능
기존 RNN/LSTM 기반 모델과의 성능 비교

9. 트랜스포머의 영향과 후속 연구
BERT, GPT, T5 등 트랜스포머의 발전
NLP 및 다양한 영역에서 트랜스포머 기반 모델이 끼친 영향

10. 결론 및 요약
트랜스포머의 주요 기여 요약
어텐션 기반 모델이 NLP 작업에 미친 혁신적 변화
미래 전망 및 트랜스포머의 응용 가능성
