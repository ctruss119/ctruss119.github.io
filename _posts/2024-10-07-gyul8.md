---
title:  "10월/ 3. BERT 공부하기 - BERT: Pre-training of Deep Bidirectional Transformers for
 Language Understanding "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">1. BERT 모델 개요</a></li>
  <li><a href="#section2">2. 허깅페이스 코드 분석</a></li>
  <li><a href="#section3">3. BERT의 핵심 개념 및 함수 분석</a></li>
</ul>

---
일단 [논문](https://arxiv.org/pdf/1810.04805) 공부하고...

# 1. BERT 모델 개요 <a name="section1"></a>
* BERT의 목적과 구조
  - 마스크드 언어 모델링 (MLM)과 다음 문장 예측 (NSP)
  - 사전 훈련(pre-training)과 미세 조정(fine-tuning)
* BERT와 트랜스포머의 관계
  - BERT의 트랜스포머 기반 구조
