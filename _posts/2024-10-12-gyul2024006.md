---
title:  "10월/ 3. BERT 공부하기(1)- BERT: Pre-training of Deep Bidirectional Transformers for
 Language Understanding 논문 리뷰 "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section0">0. Abstract. </a></li>
  <li><a href="#section1">1. Introduction. </a></li>
  <li><a href="#section2">2. Related Work </a></li>
  <li><a href="#section2.1">2.1 Unsupervised Feature-based Approaches </a></li>
  2.2 Unsupervised Fine-tuning Approaches
  2.3 Transfer Learning from Supervised Data

  3. BERT
</ul>

---
 [논문](https://arxiv.org/pdf/1810.04805) 

# 0. Abstract. <a name="section0"></a>
이 논문은 BERT(Bidirectional Encoder Representations from Transformers)라는 새로운 언어 표현 모델을 소개한다. 최근의 언어 표현 모델(Peters, Radford)과는 달리, BERT는 **모든 층에서 좌측과 우측 문맥을 공동으로 조건화하여 비지도 학습된 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계**되었다. 그 결과, 사전 훈련된 BERT 모델은 추가적인 출력 층 하나만으로도 질문 응답이나 언어 추론과 같은 다양한 작업에서 최첨단 모델을 만들 수 있으며, 작업에 특화된 아키텍처의 수정이 거의 필요하지 않다.

BERT는 개념적으로 간단하면서도 경험적으로 강력하다. BERT는 열한 가지 자연어 처리 작업에서 새로운 최첨단 결과를 얻었으며, GLUE 점수를 80.5%로 끌어올렸고(절대 7.7% 포인트 향상), MultiNLI 정확도를 86.7%로(절대 4.6% 향상), SQuAD v1.1 질문 응답 테스트 F1 점수를 93.2로(절대 1.5% 포인트 향상), 그리고 SQuAD v2.0 테스트 F1 점수를 83.1로(절대 5.1% 포인트 향상) 향상시켰다.


---

# 1. Introduction <a name="section1"></a>
Pre-trained Language Model 이 많은 자연어 처리 작업의 성능을 향상시키는 데 효과적이라는 것이 입증되었다. 

**자연어 추론**이나 **패러프레이징**과 같이 문장을 전체적으로 분석하여 문장 간의 관계를 예측하는 문장 수준 작업과, 명명된 개체 인식이나 질문 응답처럼 토큰 수준에서 세밀한 출력을 요구하는 토큰 수준 작업이 포함된다.

사전 훈련된 언어 표현을 다운스트림 작업에 적용하는 두 가지 기존 전략이 있다: feature-based 접근법과 fine-tuning 접근법. 

feature-based 접근법은 사전 훈련된 표현을 추가적인 특징으로 포함하는 작업별 아키텍처를 사용한다. (ELMo etc.)
fine-tuning 접근법은 최소한의 작업별 매개변수를 도입하고, 단순히 모든 사전 훈련된 매개변수를 미세 조정하여 다운스트림 작업에서 학습한다. (OpenAI GPT etc.) 
두 접근법은 사전 훈련 중 동일한 목표 함수를 공유하며, 단방향 언어 모델을 사용하여 일반적인 언어 표현을 학습한다.

우리는 현재 기술이 특히 fine-tuning 접근법에서 사전 훈련된 표현의 잠재력을 제한한다고 주장한다. 주요 제한은 표준 언어 모델이 **단방향**이라는 점으로, 이는 사전 훈련 중에 사용할 수 있는 아키텍처 선택을 제한한다. 예를 들어, OpenAI GPT에서 저자들은 좌에서 우로의 아키텍처를 사용하며, Transformer의 self-attention 레이어에서 각 토큰은 이전 토큰들만 참조할 수 있다. 이러한 제한은 문장 수준 작업에서 최적이 아니며, 양방향 문맥을 포함하는 것이 중요한 질문 응답과 같은 토큰 수준 작업에서 fine-tuning 기반 접근법을 적용할 때 매우 해로울 수 있다.

이 논문에서 우리는 **BERT(Bidirectional Encoder Representations from Transformers)** 를 제안하여 fine-tuning 기반 접근법을 개선한다. BERT는 masked language model(MLM) 사전 훈련 목표를 사용하여 앞서 언급한 단방향성 제약을 해소하며, 이는 Cloze task에서 영감을 받았다. Masked language model은 입력에서 일부 토큰을 무작위로 마스킹하고, 그 문맥을 기반으로 마스킹된 단어의 원래 어휘 ID를 예측하는 것이다. 좌에서 우로의 언어 모델 사전 훈련과 달리, MLM 목표는 좌우 문맥을 융합할 수 있는 표현을 가능하게 하며, 이를 통해 깊이 있는 양방향 Transformer를 사전 훈련할 수 있다. 또한, 마스킹된 언어 모델 외에도 "다음 문장 예측" 작업을 사용하여 텍스트 쌍의 표현을 함께 사전 훈련한다.

이 논문의 기여는 다음과 같다:
우리는 언어 표현을 위한 양방향 사전 훈련의 중요성을 입증한다. Radford과는 달리, **BERT는 깊이 있는 양방향 표현을 사전 훈련할 수 있게 하는 마스킹된 언어 모델**을 사용한다. 이는 Peters가 독립적으로 훈련된 좌에서 우, 우에서 좌 LMs를 얕게 연결한 것과도 대조된다.
우리는 사전 훈련된 표현이 많은 작업에 특화된 아키텍처의 필요성을 줄여준다는 것을 보여준다. BERT는 문장 수준과 토큰 수준 작업 모두에서 최첨단 성능을 달성한 첫 번째 fine-tuning 기반 표현 모델로, 많은 작업별 아키텍처를 능가한다. BERT는 11가지 NLP 작업에서 최첨단 성능을 크게 향상시켰다.

# 2.  Related Work <a name="section2"></a>
일반 언어 표현을 사전 훈련하는 역사는 길며, 이 섹션에서는 가장 널리 사용되는 접근법들을 간략히 검토한다.

## 2.1  Unsupervised Feature-based Approaches <a name="section2.1"></a>
수십 년간, 단어의 널리 적용 가능한 표현을 학습하는 것은 활발한 연구 분야였으며, 비신경망과 신경망 방법이 포함된다. 사전 훈련된 단어 임베딩은 현대 NLP 시스템의 중요한 부분으로, 처음부터 학습된 임베딩에 비해 상당한 성능 향상을 제공한다. 단어 임베딩 벡터를 사전 훈련하기 위해 좌에서 우로의 언어 모델링 목표와 좌우 문맥에서 올바른 단어와 잘못된 단어를 구별하는 목표가 사용되었다.

이러한 접근법들은 문장 임베딩 또는 문단 임베딩과 같은 더 큰 단위로 일반화되었다. 문장 표현을 학습하기 위해, 이전 연구는 후보 다음 문장을 순위 매기는 목표, 이전 문장의 표현을 기반으로 다음 문장의 단어들을 좌에서 우로 생성하는 목표, 또는 노이즈 제거 autoencoder에서 파생된 목표를 사용했다.

ELMo와 그 전신은 전통적인 단어 임베딩 연구를 다른 차원으로 확장한다. 이들은 좌에서 우로의 언어 모델과 우에서 좌로의 언어 모델에서 문맥에 민감한 특징을 추출한다. 각 토큰의 문맥적 표현은 좌에서 우와 우에서 좌 표현을 연결한 것이다. ELMo는 기존 작업별 아키텍처와 문맥적 단어 임베딩을 통합할 때, 질문 응답, 감정 분석, 명명된 개체 인식 등 주요 NLP 벤치마크에서 최첨단 성능을 달성한다. Melamud은 좌우 문맥을 사용해 단어 하나를 예측하는 작업을 통해 문맥적 표현을 학습하는 방법을 제안했다. 이들의 모델은 ELMo와 마찬가지로 feature-based이며 깊이 있는 양방향성은 없다. Fedus는 cloze task가 텍스트 생성 모델의 강인성을 향상시킬 수 있음을 보여준다.


## 2.2 Unsupervised Fine-tuning Approaches  <a name="section2.2"></a>
 Feature-based 접근법과 마찬가지로, 이 방향의 초기 작업들은 비지도 텍스트에서 단어 임베딩 매개변수만을 사전 훈련했다.최근에는 문장 또는 문서 인코더가 비지도 텍스트로부터 문맥적 토큰 표현을 생성하도록 사전 훈련되고, 이후 지도 학습 기반 다운스트림 작업에 fine-tuning되는 방식이 사용되고 있다. 이러한 접근법의 장점은 처음부터 학습해야 할 매개변수가 적다는 것이다. 이 장점 덕분에 OpenAI GPT는 GLUE benchmark의 많은 문장 수준 작업에서 이전의 최첨단 결과를 달성할 수 있었다. 좌에서 우로의 언어 모델링과 auto-encoder 목표가 이러한 모델들의 사전 훈련에 사용되었다.

그림 1: BERT의 전체 사전 훈련 및 fine-tuning 절차를 나타낸다. 출력 레이어를 제외하고, 사전 훈련과 fine-tuning에서 동일한 아키텍처가 사용된다. 동일한 사전 훈련된 모델 매개변수가 다양한 다운스트림 작업의 모델을 초기화하는 데 사용된다. Fine-tuning 중에는 모든 매개변수가 미세 조정된다. **[CLS]**는 모든 입력 예제 앞에 추가되는 특수 기호이며, **[SEP]**는 질문/답변을 구분하는 특수 구분 토큰이다.

## 2.3 Transfer Learning from Supervised Data  <a name="section2.3"></a>
또한 대규모 데이터셋을 사용하는 지도 학습 작업에서 효과적인 전이 학습이 가능하다는 연구들도 있다. 예를 들어, 자연어 추론(Conneau et al., 2017)과 기계 번역(McCann et al., 2017)이 이에 해당한다. Computer vision 연구에서도 대규모 사전 훈련된 모델로부터 전이 학습이 중요함이 입증되었으며, ImageNet(Deng et al., 2009; Yosinski et al., 2014)으로 사전 훈련된 모델을 fine-tuning하는 것이 효과적인 방법이라는 것이 나타났다.

# 3. BERT<a name="section3"></a>
이 섹션에서는 BERT와 그 세부 구현에 대해 설명한다. 우리 프레임워크는 pre-training과 fine-tuning의 두 단계로 이루어져 있다. Pre-training 단계에서는 비지도 데이터에서 다양한 사전 훈련 작업을 통해 모델을 학습시킨다. Fine-tuning 단계에서는 BERT 모델이 사전 훈련된 매개변수로 초기화된 후, 다운스트림 작업의 라벨이 있는 데이터를 사용하여 모든 매개변수를 미세 조정한다. 각 다운스트림 작업은 별도의 fine-tuning된 모델을 가지지만, 동일한 사전 훈련된 매개변수로 초기화된다. 그림 1에 나오는 질문-답변 예시는 이 섹션의 계속되는 예시로 사용될 것이다.

BERT의 독특한 특징 중 하나는 다양한 작업에서 통일된 아키텍처를 사용한다는 점이다. 사전 훈련된 아키텍처와 최종 다운스트림 아키텍처 간의 차이가 최소화된다.

### 모델 아키텍처
BERT의 모델 아키텍처는 다중 레이어 bidirectional Transformer 인코더로, Vaswani et al. (2017)에서 설명된 원래의 구현을 기반으로 하였고 tensor2tensor library에서 제공되었다. Transformer의 사용은 이미 일반적이며, 우리의 구현은 원래와 거의 동일하므로, 모델 아키텍처에 대한 배경 설명은 생략하고 Vaswani et al. (2017) 및 훌륭한 가이드인 "The Annotated Transformer"를 참고하기 바란다.

이 작업에서는 레이어의 수(즉, Transformer 블록)를 L, 히든 크기를 H, 그리고 self-attention 헤드의 수를 A로 나타낸다. 주로 두 가지 모델 크기에 대한 결과를 보고하는데, BERTBASE(L=12, H=768, A=12, 총 매개변수=110M)와 BERTLARGE(L=24, H=1024, A=16, 총 매개변수=340M)가 그것이다. BERTBASE는 비교 목적으로 OpenAI GPT와 동일한 모델 크기를 가지도록 선택되었다. 하지만 중요한 점은 BERT의 Transformer가 bidirectional self-attention을 사용하는 반면, GPT Transformer는 모든 토큰이 좌측의 문맥에만 주목할 수 있는 제약된 self-attention을 사용한다.

### 입력/출력 표현
BERT가 다양한 다운스트림 작업을 처리할 수 있도록, 우리의 입력 표현은 단일 문장과 문장 쌍(예: 질문, 답변)을 하나의 토큰 시퀀스로 명확하게 나타낼 수 있다. 여기서 "문장"은 실제 문장이 아닌 연속된 텍스트의 임의의 범위를 나타낼 수 있으며, "시퀀스"는 BERT로의 입력 토큰 시퀀스를 의미하며, 단일 문장 또는 두 개의 문장을 함께 묶은 시퀀스일 수 있다.

우리는 30,000개의 토큰을 가진 WordPiece 임베딩(Wu et al., 2016)을 사용한다. 모든 시퀀스의 첫 번째 토큰은 항상 특수 분류 토큰([CLS])이다. 이 토큰에 대응하는 최종 히든 상태는 분류 작업에서 시퀀스 전체의 표현으로 사용된다. 문장 쌍은 하나의 시퀀스로 묶이며, 두 가지 방식으로 문장을 구분한다. 첫째, 우리는 문장을 특수 구분 토큰([SEP])으로 구분한다. 둘째, 각 토큰에 학습된 임베딩을 추가하여 해당 토큰이 문장 A에 속하는지, 문장 B에 속하는지를 나타낸다. 그림 1에서, 우리는 입력 임베딩을 E, 특수 [CLS] 토큰의 최종 히든 벡터를 C RH, 그리고 i번째 입력 토큰의 최종 히든 벡터를 Ti RH로 나타낸다.

주어진 토큰의 입력 표현은 해당 토큰 임베딩, 세그먼트 임베딩, 위치 임베딩을 더하여 구성된다. 이 구성의 시각적 표현은 그림 2에서 확인할 수 있다.

Figure 2: BERT Input Representation
BERT의 입력 표현은 토큰 임베딩, 세그먼트 임베딩, 그리고 위치 임베딩의 합으로 구성된다.

## 3.1 Pre-training BERT BERT<a name="section3.1"></a>
 **Peters et al. (2018a)**나 **Radford et al. (2018)**와 달리, 우리는 전통적인 좌측에서 우측 또는 우측에서 좌측으로의 언어 모델을 사용하지 않고, 대신 두 가지 비지도 학습 과제를 사용하여 BERT를 사전 훈련한다. 이 단계는 그림 1의 왼쪽 부분에 나타나 있다.

### 과제 1: Masked LM
직관적으로, 깊이 있는 bidirectional 모델이 좌측에서 우측으로의 모델이나 좌측과 우측 모델의 얕은 연결보다 더 강력할 것이라고 생각할 수 있다. 그러나, 표준 conditional language models는 좌측에서 우측으로 또는 우측에서 좌측으로만 훈련될 수 있다. Bidirectional 조건을 허용하면 각 단어가 간접적으로 자기 자신을 "볼 수" 있게 되며, 다층적 문맥에서 대상 단어를 예측하는 것이 매우 간단해진다.

이를 해결하기 위해, 우리는 입력 토큰의 일부를 무작위로 mask 처리하고, 그 mask된 토큰들을 예측하는 방식을 사용한다. 이 절차를 **"Masked LM (MLM)"**이라고 부르며, 이는 문헌에서 Cloze task로 불리기도 한다(Taylor, 1953). 이 경우, mask된 토큰에 해당하는 최종 히든 벡터는 표준 LM에서와 같이 어휘에 대한 softmax 출력으로 연결된다. 우리의 모든 실험에서, 우리는 각 시퀀스에서 WordPiece 토큰의 15%를 무작위로 mask 처리한다. Denoising auto-encoders(Vincent et al., 2008)와 달리, 우리는 전체 입력을 복원하는 대신 mask된 단어만을 예측한다.

이 방식을 통해 bidirectional 사전 훈련 모델을 얻을 수 있지만, pre-training과 fine-tuning 간의 불일치가 생긴다. 왜냐하면 fine-tuning 단계에서 [MASK] 토큰이 등장하지 않기 때문이다. 이를 완화하기 위해, 우리는 "mask"된 단어를 항상 실제 [MASK] 토큰으로 대체하지 않는다. 훈련 데이터 생성기는 예측을 위해 임의로 15%의 토큰 위치를 선택한다. 선택된 토큰은 (1) 80%의 확률로 [MASK] 토큰으로 교체되며, (2) 10%의 확률로 임의의 다른 토큰으로 교체되며, (3) 10%의 확률로 원래 토큰을 그대로 유지한다. 그 후, Ti는 교차 엔트로피 손실로 원래의 토큰을 예측하는 데 사용된다. 이 절차의 변형은 부록 C.2에서 비교한다.

### 과제 2: 다음 문장 예측 (Next Sentence Prediction, NSP)
질문 응답(Question Answering, QA)과 자연어 추론(Natural Language Inference, NLI) 같은 중요한 다운스트림 작업은 두 문장 간의 관계를 이해하는 것을 기반으로 한다. 이러한 관계는 단순한 언어 모델링으로는 직접적으로 학습되지 않는다. 문장 관계를 이해하는 모델을 훈련하기 위해, 우리는 binarlized된 next sentence prediction 작업을 사용하여 사전 훈련한다. 이 작업은 단일 언어 코퍼스에서 쉽게 생성할 수 있다. 구체적으로, 각 사전 훈련 예제에서 A 문장과 B 문장을 선택할 때, 50%의 경우 B는 실제로 A 뒤에 오는 문장(레이블: IsNext)이며, 나머지 50%의 경우 B는 코퍼스에서 무작위로 선택된 문장(레이블: NotNext)이다. 그림 1에서 보이듯, C는 다음 문장 예측(NSP)에서 사용된다.

이 작업은 매우 간단하지만, 5.1절에서 보여주듯, 이 과제로 사전 훈련하는 것은 QA와 NLI 모두에 매우 유익하다. NSP 작업은 Jernite et al. (2017) 및 **Logeswaran and Lee (2018)**에서 사용된 표현 학습 목표와 밀접하게 관련되어 있다. 하지만 이전 작업에서는 문장 임베딩만 다운스트림 작업으로 전이되는 반면, BERT는 모든 매개변수를 전이하여 최종 작업 모델의 매개변수를 초기화한다.


###사전 훈련 데이터
사전 훈련 절차는 기존 언어 모델 사전 훈련 문헌을 주로 따른다. 우리는 BooksCorpus(800M 단어)와 English Wikipedia(2,500M 단어)를 사전 훈련 코퍼스로 사용한다(Zhu et al., 2015). 위키피디아의 경우, 목록, 표, 제목 등을 제외하고 텍스트 부분만 추출하여 사용한다. 긴 연속된 시퀀스를 추출하기 위해, Billion Word Benchmark(Chelba et al., 2013)와 같은 문장 수준으로 섞인 코퍼스가 아닌 문서 수준 코퍼스를 사용하는 것이 중요하다.

## 3.2 Fine-tuning BERT  <a name="section3.2"></a>
 BERT의 파인튜닝은 매우 직관적이다. Transformer의 self-attention 메커니즘 덕분에, BERT는 단일 텍스트나 텍스트 쌍을 포함한 다양한 다운스트림 작업을 모델링할 수 있다. 이는 적절한 입력과 출력을 바꾸는 것으로 가능하다.

텍스트 쌍을 처리하는 응용 프로그램에서는 텍스트 쌍을 독립적으로 인코딩한 후 bidirectional cross-attention을 적용하는 것이 일반적인 패턴이다(예: Parikh et al. (2016), Seo et al. (2017)). 하지만 BERT는 self-attention 메커니즘을 활용해 이 두 단계를 통합한다. self-attention을 사용해 연결된 텍스트 쌍을 인코딩하면 두 문장 간의 bidirectional cross-attention이 자연스럽게 포함된다.

각 작업마다 BERT에 해당 작업의 입력과 출력을 연결하고, 모든 매개변수를 끝까지 fine-tune 한다. 입력 단계에서, 사전 훈련에서 사용된 sentence A와 sentence B는 다음과 같은 텍스트 쌍으로 대응된다:

paraphrasing에서의 문장 쌍
entailment에서의 가설-전제 쌍
question answering에서의 질문-지문 쌍
텍스트 분류나 시퀀스 태깅에서의 단순한 텍스트 쌍
출력 단계에서는 token-level 작업(예: sequence tagging 또는 question answering)에 대해 토큰 표현이 출력층으로 전달되며, [CLS] 표현은 classification 작업(예: entailment 또는 sentiment analysis)에 대한 출력층으로 전달된다.

Pre-training과 비교하면, fine-tuning은 상대적으로 비용이 적게 든다. 이 논문에 제시된 모든 결과는 동일한 사전 훈련 모델에서 시작하여, Cloud TPU에서는 최대 1시간, GPU에서는 몇 시간 내에 재현 가능하다. 해당 작업별 세부 사항은 4장에 설명되어 있으며, 더 많은 정보는 부록 A.5에 나와 있다.


# 4. Experiments <a name="section4"></a>
 In this section, we present BERT fine-tuning re
sults on 11 NLP tasks.
##  4.1 GLUE <a name="section4.1"></a>
GLUE(General Language Understanding Evaluation) 벤치마크( Wang et al., 2018a)는 다양한 자연어 이해 작업의 집합이다. GLUE 데이터셋에 대한 자세한 설명은 부록 B.1에 포함되어 있다.

GLUE에서의 파인튜닝을 위해, 입력 시퀀스(단일 문장 또는 문장 쌍)는 3장에서 설명한 대로 표현하며, 첫 번째 입력 토큰 ([CLS])에 해당하는 최종 은닉 벡터 
𝐶∈𝑅^𝐻 를 집합 표현으로 사용한다. 파인튜닝 중에 도입되는 유일한 새로운 매개변수는 분류 레이어 가중치 𝑊∈𝑅^(K*𝐻)이다. 여기서 𝐾는 레이블의 수이다. 우리는 𝐶와 𝑊를 사용하여 표준 분류 손실을 계산한다:

loss=log(softmax(𝐶𝑊^(𝑇))

배치 크기는 32로 설정하고, 모든 GLUE 작업에 대해 3 에포크 동안 데이터에 대해 파인튜닝을 수행한다. 각 작업에 대해 Dev set에서 최적의 파인튜닝 학습률(5e-5, 4e-5, 3e-5, 2e-5 중)을 선택한다. 또한, BERTLARGE의 경우, 작은 데이터셋에서 파인튜닝이 불안정한 경우가 있었기 때문에 여러 번의 랜덤 재시작을 수행하고 Dev set에서 최적의 모델을 선택한다. 랜덤 재시작에서는 동일한 사전 훈련 체크포인트를 사용하되, 서로 다른 파인튜닝 데이터 셔플링과 분류기 레이어 초기화를 수행한다.

결과는 표 1에 제시되어 있다. BERTBASE와 BERTLARGE 모두 모든 작업에서 다른 시스템에 비해 상당한 차이로 성능을 초과하며, 각각 4.5%와 7.0%의 평균 정확도 향상을 달성하였다. BERTBASE와 OpenAI GPT는 주의 마스킹을 제외하고 모델 아키텍처 측면에서 거의 동일하다. 가장 크고 널리 보고된 GLUE 작업인 MNLI에서 BERT는 4.6%의 절대 정확도 향상을 얻었다. 공식 GLUE 리더보드에서 BERTLARGE는 80.5의 점수를 얻었으며, OpenAI GPT는 작성 당시 72.8의 점수를 얻었다.

BERTLARGE는 특히 훈련 데이터가 매우 적은 작업에서 모든 작업에 걸쳐 BERTBASE를 상당히 초과 성능을 보였다. 모델 크기의 효과는 5.2 섹션에서 보다 자세히 탐색된다.

##  4.2 SQuADv1.1 GLUE <a name="section4.2"></a>
Stanford Question Answering Dataset (SQuADv1.1)은 100,000개의 크라우드 소싱된 질문/답변 쌍의 모음이다 (Rajpurkar et al., 2016). 주어진 질문과 해당 답변이 포함된 위키피디아의 구절에서, 작업은 구절 내에서 답변 텍스트 범위를 예측하는 것이다. 그림 1에서 보듯이, 질문 응답 작업에서 우리는 입력 질문과 구절을 단일 패킹된 시퀀스로 표현하며, 질문은 A 임베딩을 사용하고 구절은 B 임베딩을 사용한다. 우리는 파인튜닝 동안 시작 벡터 S_RH와 종료 벡터 E_RH만 도입한다.

단어 i가 답변 범위의 시작점이 될 확률은 Ti와 S 간의 내적(dot product)과 구절 내 모든 단어에 대한 소프트맥스를 통해 계산된다: 
𝑃
𝑖
=
𝑒
𝑆
𝑇
𝑇
𝑖
∑
𝑗
𝑒
𝑆
𝑇
𝑇
𝑗
P 
i
​
 = 
∑ 
j
​
 e 
S 
T
 T 
j
​
 
 
e 
S 
T
 T 
i
​
 
 
​
 . 종료 지점의 경우도 유사한 공식을 사용한다. 후보 범위의 점수는 위치 i에서 위치 j까지 정의되며, 
𝑆
𝑇
𝑇
𝑖
+
𝐸
𝑇
𝑇
𝑗
S 
T
 T 
i
​
 +E 
T
 T 
j
​
 로 표현된다. 가장 높은 점수를 얻는 범위는 j > i를 기준으로 예측에 사용된다. 훈련 목표는 올바른 시작 및 종료 위치의 로그 우도(log-likelihood)의 합이다. 우리는 3 epochs 동안 5e-5의 학습률과 32의 배치 크기로 파인튜닝을 수행하였다.

표 2는 최상위 리더보드 항목 및 상위 출판 시스템(Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018)의 결과를 보여준다. SQuAD 리더보드의 최고 결과는 최신 공개 시스템 설명이 없으며, 시스템 훈련 시 모든 공개 데이터를 사용할 수 있다. 따라서 우리는 TriviaQA(Joshi et al., 2017)에서 파인튜닝을 먼저 수행한 후 SQuAD에서 파인튜닝을 통해 적당한 데이터 증강을 사용한다. 우리의 최상위 성능 시스템은 단일 시스템으로 +1.5 F1의 앙상블을 초과하고, +1.3 F1의 성능을 보인다. 사실, 우리의 단일 BERT 모델은 F1 점수 측면에서 최고 앙상블 시스템을 초과한다. TriviaQA 파인튜닝 데이터 없이도, 우리는 0.1-0.4 F1 점수를 잃고도 여전히 모든 기존 시스템을 큰 차이로 초과한다.


##  4.3 SQuAD v2.0 <a name="section4.3"></a>
SQuAD v2.0 작업은 제공된 문단에서 짧은 답변이 존재하지 않을 가능성을 허용하여 SQuAD v1.1 문제 정의를 확장하여 보다 현실적인 문제를 만든다. 우리는 이 작업을 위해 SQuAD v1.1 BERT 모델을 확장하는 간단한 접근 방식을 사용한다. 답변이 없는 질문은 [CLS] 토큰에서 시작하고 끝나는 답변 범위가 있다고 간주한다. 답변의 시작과 끝 위치에 대한 확률 공간은 [CLS] 토큰의 위치를 포함하도록 확장된다.

예측을 위해, 답변이 없다는 범위의 점수 
𝑠
null
=
𝑆
𝐶
+
𝐸
𝐶
s 
null
​
 =S 
C
​
 +E 
C
​
 와 가장 좋은 비-널 범위의 점수 
𝑠
𝑖
𝑗
=
max
⁡
𝑗
(
𝑆
𝑖
+
𝐸
𝑗
)
s 
ij
​
 =max 
j
​
 (S 
i
​
 +E 
j
​
 )를 비교한다. 우리는 다음 조건을 만족할 때 비-널 답변을 예측한다:

𝑠
𝑖
𝑗
>
𝑠
null
+
𝛿
s 
ij
​
 >s 
null
​
 +δ
여기서 임계값 
𝛿
δ는 Dev set에서 F1 점수를 최대화하도록 선택된다. 우리는 이 모델에 대해 TriviaQA 데이터를 사용하지 않았다. 학습률 5e-5로 2 에포크 동안 파인튜닝을 진행했으며, 배치 크기는 48로 설정하였다.

결과는 표 3에 제시되어 있으며, 여기에는 BERT를 구성 요소로 사용하는 시스템은 제외된다. 우리는 이전 최고 시스템에 비해 +5.1의 F1 점수 개선을 관찰하였다.


##  4.4 SWAG <a name="section4.4"></a>
SWAG (Situations With Adversarial Generations) 데이터셋은 113,000개의 문장 쌍 완성 예제를 포함하여 기초적인 상식 추론을 평가한다 (Zellers et al., 2018). 주어진 문장에 대해, 작업은 네 가지 선택 중에서 가장 그럴듯한 연속성을 선택하는 것이다.

SWAG 데이터셋에서 파인튜닝을 할 때, 우리는 주어진 문장(문장 A)과 가능한 연속성(문장 B)의 연결로 구성된 네 개의 입력 시퀀스를 만든다. 오직 문서 특정 파라미터로는 [CLS] 토큰 표현 
𝐶
C와의 내적을 통해 각 선택에 대한 점수를 나타내는 벡터만 도입된다. 이 점수는 소프트맥스 레이어를 통해 정규화된다.

모델은 2e-5의 학습률과 배치 크기 16으로 3 에포크 동안 파인튜닝되었다. 결과는 표 4에 제시되어 있으며, BERTLARGE는 저자들이 제안한 기본 모델인 ESIM+ELMo 시스템에 비해 27.1% 높은 성능을 보였고, OpenAI GPT보다 8.3% 향상된 결과를 나타냈다.






////
# 5.  <a name="section5"></a>
이 섹션에서는 BERT의 여러 측면에 대한 제거 실험을 수행하여 각 요소의 상대적 중요성을 더 잘 이해하고자 한다. 추가적인 제거 연구는 부록 C에서 확인할 수 있다. 
# 6.  <a name="section6"></a>



