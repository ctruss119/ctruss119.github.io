---
title:  "10월/ 1. 트랜스포머 공부하기(1) - Attention Is All You Need 논문 읽기 "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section0">0. Abstract.  </a></li>   
  <li><a href="#section1">1. Introduction  </a></li>   
  <li><a href="#section2">2. Background  </a></li>
  
  <li><a href="#section3">3. ModelArchitecture </a></li>
  <li><a href="#section4">3.1 Encoder and Decoder Stacks </a></li>
  <li><a href="#section5">3.2 Attention </a></li>
  <li><a href="#section6">3.3 Position-wise Feed-Forward Networks </a></li>
  <li><a href="#section7">3.4 Embeddings and Softmax </a></li>
  <li><a href="#section8">3.5 Positional Encoding </a></li>
  
  <li><a href="#section9">4.  WhySelf-Attention </a></li>
  
  <li><a href="#section10">5.  Training </a></li>
  <li><a href="#section11">6.  Results </a></li>
  <li><a href="#section12">7.  Conclusion </a></li>
  <li><a href="#section13">8.  정리 </a></li>  
</ul>

-------------------------------------------------------------------   

# 0. 
[Attention Is All You Need 논문링크](https://arxiv.org/pdf/1706.03762)
가능한 원문 그대로 읽어서 기록흠   
모르거나 헷갈리는 개념이 나오면 **볼드**처리 후 각 단락마다 맨 밑에 설명을 작성함 설명하면서 작성함    
* 중간중간 이렇게 처리한 건 잘 이해가 안 되는 부분 정리해서 첨삭한 내용임 

/// ppt 로 구성하면서 이 포스트에서 틀리거나 잘못된 부분, 생략된 부분이 많다는 걸 깨닫고 다시 공부했음.      
[[최종]트랜스포머김규리.pptx](https://github.com/user-attachments/files/17338151/default.pptx)
기록용으로 ppt를 첨부해두니 구글링 등으로 찾아온 사람에게 도움이 되길 바람... 
---
# <a id="section0"></a>0.  Abstract. 

  현재 많이 쓰이는 **시퀀스 변환 모델**은 복잡한 **순환 신경망**(Recurrent Neural Networks, RNN) 또는 **합성곱 신경망**(Convolutional Neural Networks, CNN)을 기반으로 하며,  **인코더**와 **디코더**를 포함하고 있다. 가장 성능이 좋은 모델들은 인코더와 디코더를 **어텐션 메커니즘**을 통해 연결한다. 

이 논문에서는 이러한 복잡성을 없애고 순수하게 어텐션 메커니즘만을 기반으로 한 새로운 간단한 네트워크 아키텍처인 트랜스포머(Transformer)를 제안한다. 이 모델은 순환(RNN)이나 합성곱(CNN)을 완전히 배제한다. 두 가지 **기계 번역 작업**에 대한 실험 결과, 이 모델(트랜스포머)이 기존의 모델들보다 더 우수한 품질을 보이며, **병렬화 가능성**이 더 높고 훈련 시간이 현저히 적게 소요된다는 것을 확인했다. 

이 논문에 실린 트랜스포머 모델은 2014년 WMT 영어-독일어 번역 작업에서 28.4 BLEU 점수를 기록했으며, 이는 기존 최고 성능을 보였던 모델들(앙상블 포함)보다 2 **BLEU** 이상 개선된 결과이다. 또한, 2014년 WMT 영어-프랑스어 번역 작업에서는 41.8 BLEU로 단일 모델 기준 새로운 최고 성능을 기록했으며, 8개의 GPU에서 3.5일 동안 훈련한 결과, 기존 문헌에 나와 있는 최고 성능 모델들보다 훈련 비용이 매우 적었다. 우리는 트랜스포머가 영어 구문 분석(constituency parsing)에도 잘 적용된다는 것을 확인했으며, 대규모 데이터셋과 제한된 데이터셋 모두에서 성공적인 결과를 보였다. 

-------------- 
* 시퀀스 변환 모델: 입력 시퀀스를 받아서 출력 시퀀스로 변환하는 모델. 예를 들어, 기계 번역에서 입력된 문장을 다른 언어로 변환하는 작업이 이에 해당한다. 자연어 처리(NLP)에서 자주 사용되며, 번역뿐 아니라 음성 인식, 요약, 텍스트 생성 같은 작업에 활용된다.   

* 순환 신경망(RNN): 순차적 데이터(시퀀스)를 처리하기 위해 설계된 신경망. 시퀀스의 각 단계에서 이전 단계의 출력을 다음 단계로 전달해, 이전 정보를 기억하고 현재 입력을 처리할 수 있다. 주로 자연어 처리에서 사용되며, 텍스트 생성, 번역, 음성 인식 등에 활용된다. 시퀀스가 길어지면 정보가 소실되거나 장기 의존 관계(long-term dependencies)를 학습하기 어려운 단점이 있다.   

* 합성곱 신경망(CNN): 공간적 데이터(이미지)를 처리하는 데 주로 사용되며, 특징 추출에 탁월한 성능을 보입니다. 입력 데이터를 여러 필터를 통해 합성곱(Convolution) 연산을 적용하여 중요한 특징을 추출하고, 이를 통해 다양한 작업을 수행한다. 최근에는 텍스트 처리 작업에도 CNN이 활용되고 있다. 주로 이미지 인식 및 영상 처리에 사용되지만, NLP에서도 합성곱을 통해 문장의 패턴을 추출하는 방식으로 응용된다.   

* 인코더: 입력 데이터를 벡터로 변환하는 네트워크. NLP 작업에서 인코더는 입력 시퀀스를 처리하여, 그 의미를 함축한 벡터 표현을 생성한다. 예를 들어, 문장을 하나의 고차원 벡터로 변환하는 역할을 한다.   
 
* 디코더: 인코더에서 생성된 압축된 벡터 표현을 입력받아, 이를 바탕으로 출력 시퀀스를 생성하는 역할. 주로 번역 작업에서, 인코더가 입력된 문장을 벡터로 변환한 후, 디코더가 해당 벡터를 사용하여 목표 언어로 변환된 문장을 출력한다.
 
 * 어텐션 매커니즘: 입력 시퀀스의 각 요소를 처리할 때 다른 중요한 요소에 더 많은 가중치를 부여하여 처리하는 기법. 특히, 문장 내에서 중요한 단어에 집중하여 그 관계를 더 잘 파악할 수 있도록 도와준다. 트랜스포머에서 어텐션 매커니즘ㅁ에 집중한 이유는, RNN 등의 모델은 시퀀스가 길어지면 정보를 잊는 문제가 있는데, 어텐션 메커니즘은 모든 입력 요소를 한 번에 살펴보며 중요한 정보를 놓치지 않게 한다. ... 논문 더 읽다보면 자세히 나옴 

* 기계 번역 작업: 한 언어의 텍스트를 다른 언어로 자동으로 변환하는 작업. NLP의 대표적인 응용 분야로, 딥러닝과 트랜스포머 모델은 기계 번역의 성능을 크게 향상시켰다. 예시로, 영어에서 독일어로 문장을 번역하는 작업이 있다.   
   
* 병렬화 가능성: 모델이 여러 작업을 동시에 처리할 수 있는 능력. RNN은 순차적으로 데이터를 처리하기 때문에 병렬화가 어렵지만, 트랜스포머는 병렬로 데이터를 처리할 수 있어, 더 빠른 훈련 속도를 가잔다. 병렬화가 잘 이루어지면 훈련 속도가 크게 향상되며, 대규모 데이터를 더 효율적으로 처리할 수 있다.

* BLEU: 기계 번역의 품질을 평가하기 위해 사용되는 지표. 번역된 문장이 정답 문장과 얼마나 유사한지를 평가하는데, 주로 단어 간의 유사성과 정확도를 측정한다. BLEU 점수는 0에서 100까지의 범위를 가지며, 점수가 높을수록 번역된 문장이 원문에 가깝다는 것을 의미한다.


# <a id="section1"></a>1.  Introduction

순환 신경망(Recurrent Neural Networks, RNN), 장단기 기억(Long Short-Term Memory, LSTM), 게이트 순환 신경망(Gated Recurrent Neural Networks, GRU)은 언어 모델링 및 기계 번역과 같은 sequence modeling 및 transduction 문제애서 sota로 자리를 잡았다. 이후 많은 연구들이 순환 언어 모델과 인코더-디코더 아키텍처의 경계를 확장하기 위해 지속적으로 노력해 왔다.   

순환 모델(Recurrent model)은 일반적으로 input 및 output sequence의 각 위치에 따라 계산을 분리하여 처리한다. 시퀀스의 각 위치(symbol positions)를 steps와 위치에 따라 정렬하여, 이전 은닉 상태(hidden state )\( h_{t-1}\)와 position t가 input인 hidden state \( h_t \)를 생성한다.    

* 순차적인 계산에 대해 풀어서 설명하자면: 순환 신경망(RNN)과 같은 모델은 입력 시퀀스의 각 단어(또는 토큰)를 시간 순서대로 처리한다. 즉, 시퀀스의 각 위치(예: t번째 단어)를 차례로 처리하며, 이전 시점의 계산 결과(은닉 상태)를 기억한 상태에서 현재 시점의 입력을 반영해 계산을 수행함. 즉슨 시퀀스의 t번째 위치에서 출력이 결정될 때, t-1번째 위치의 정보(은닉 상태)와 현재 t번째 입력이 모두 반영됨.   

이러한 순차적 처리 특성(ht-1 처리 후에야 ht를 처리할 수 있는 순차적인 특성)은 병렬 처리를 어렵게 만드는데, 이는 시퀀스 길이가 길어질수록 메모리 제한으로 인해 여러 예제를 배치(batch)로 처리하는 데 한계를 겪게 된다. 최근 연구는 계산 효율성을 개선하기 위해 인수분해 기법과 조건부 계산을 사용하여 큰 발전을 이루었으며, 후자는 모델 성능도 향상시켰다. 그러나 순차적 계산의 근본적인 제약은 여전히 남아 있다.   

* 병렬화가 어려워지는 이유: 이전 시점의 계산이 끝나야 다음 시점의 계산을 할 수 있기 때문에 병렬로 처리하기 어렵다는 단점이 있다. 즉, 시퀀스의 각 요소가 독립적으로 동시에 처리되지 않고, 하나씩 차례로 계산되어야 한다는 것을 의미함. 요약하자면 각 시퀀스 위치에서의 계산이 순차적으로 일어나며 이전 결과에 의존한다는 단점이 있다는 것이다. 

어텐션 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링과 변환 모델의 필수적인 부분이 되었다. 이는 입력 또는 출력 시퀀스에서 거리와 상관없이 의존성을 모델링할 수 있게 해준다. 하지만 일부 경우를 제외하고, 이러한 어텐션 메커니즘은 대부분 순환 신경망과 함께 사용된다. 

* 아무튼 순환 신경망이랑 섞어 쓰니까 효율적인 병렬화가 불가능하다는 것을 의미함 

본 연구에서는 순환 구조(recurrence)를 제거하고 전적으로 어텐션 메커니즘에 의존하여, 입력과 출력 간의 전역 의존성을 모델링하는 트랜스포머(Transformer)를 제안한다. 

트랜스포머는 병렬화 가능성을 크게 향상시켰으며, 8개의 P100 GPU에서 12시간 만에 훈련하여 번역 품질에서 새로운 최고 성능에 도달할 수 있었다.

* 참고로, cpu와 gpu로 본 병렬화 차이를 시각적으로 이해시켜주는 [영상](https://www.youtube.com/watch?v=1BAZf3PsjWA0) ... rnn은 병렬화가 안되니까 gpu를 잘 활용을 못함 ㅠㅠ 

# <a id="section2"></a>2. Background

순차적인 계산을 줄이려는 모델(Extended Neural GPU, ByteNet, ConvS2S 등)이 있었다. 이들 모두 합성곱 신경망 CNN(Convolutional Neural Networks)을 기본 구성 요소로 사용하여, 입력 및 출력의 모든 위치에 대해 병렬로  hidden representations를 계산한다. 이러한 모델들에서, 임의의 두 입력 또는 출력 위치 간의 신호를 연결하는 데 필요한 연산 수는 그 위치 간의 거리에 따라 증가한다. ConvS2S의 경우에는 선형적으로, ByteNet은 로그 형태로 증가한다. 이는 멀리 떨어진 위치 간의 의존성을 학습하는 것을 더 어렵게 만든다.   

트랜스포머에서는 이를 상수 개수의 연산으로 줄였지만, 어텐션 가중치가 적용된 위치의 평균으로 인해 해상도가 감소하는 문제가 발생할 수 있다. 이 문제는 3.2절에서 설명하는 멀티-헤드 어텐션(Multi-Head Attention)으로 보완된다.   
Self-attention(intra-attention)은 단일 시퀀스의 다양한 위치 간의 관계를 계산하여 그 시퀀스의 표현을 생성하는 어텐션 메커니즘입니다. Self-attention은 독해(Reading Comprehension), 요약(Abstractive Summarization), 텍스트 함의(Textual Entailment), 작업 독립적 문장 표현 학습 등 다양한 작업에서 성공적으로 사용되었다.

End-to-end 메모리 네트워크는 sequence aligned recurrence(시퀀스 정렬 순환) 대신에 순환 어텐션 메커니즘(recurrent attention mechanism)을 기반으로 하며, 간단한 언어 질문 응답(single-language QA) 및 언어 모델링 작업에서 좋은 성능을 보인다.

한편, 트랜스포머는 RNN이나 CNN을 사용하지 않고 완전히 Self-attention만을 사용하여 입력과 출력의 표현을 계산하는 최초의 변환(transduction) 모델이다. 

다음 섹션에서는 트랜스포머에 대해 설명하고, Self-attention을 어떻게 동작하게 하고 기존의 모델들보다 가지는 장점에 대해 논의할 것이다.   



# <a id="section3"></a>3.  ModelArchitecture

대부분의 경쟁력 있는 신경망 기반 시퀀스 변환 모델은 **인코더-디코더** 구조를 가지고 있다.   

인코더는 입력 시퀀스인 (x1,...,xn)의 기호 표현을 연속된 표현 시퀀스 z = (z1,...,zn)로 변환한다.  
디코더는 z가 주어지면 하나씩 출력 시퀀스 (y1,...,ym)를 생성한다.   

이러한 인코더-디코더 모델은 각 단계에서 자동회귀적(auto-regressive)으로 동작하며, 이전에 생성된 output을 다음 output을 생성할 때 추가 입력으로 사용한다. 

* 여기서 트랜스포머 이전 인코더-디코더 구조의 모델들이 이전 단계가 완료된 후에 다음단계를 수행할 수 있는, 즉 병렬적으로 처리가 불가능한 구조였음을 알 수 있다. 

트랜스포머는 이 전체적인 구조를 따라간다.   
인코더와 디코더에서 **스택으로 쌓인 self attention**과 **위치별로 완전히 연결된 계층(point-wise, fully connected layers)**을 사용하면서.

* 이게 무슨 뜻이냐면... 아직 이해가 잘 안 되기는 하는데... 트랜스포머 모델은 인코더와 디코더의 기본 구조를 유지하고 있음. 그리고, point-wise, fully connected layers 즉 위치별로 입력을 각각 독립적으로 처리하는데 또 완전히 연결된 구조(?)의 계층이 있음. 이게 층층이 쌓여 있음. self attention이 각 계층이 다른 계층들과 상호작용하면서 관게를 계산하고, 가중치 부여하고, 의존성을 모델링함. 얘네가 쌓아올려져있는 형태로 아키텍쳐가 구성되어있다는 뜻임. 사진을 보면 더 잘 이해할 수 있음.



# <a id="section4"></a>3.1  Encoder and Decoder Stacks

### Encorder
N = 6개의 동일한 레이어로 구성된다.   
각 레이어는 두 개의 서브 레이어를 갖고 있다.   
첫 번째: multi-head self-attention 메커니즘,   
두 번째: 간단한, 위치별, 완전히 연결된 feed-forward network.   

각 서브 레이어는 **잔차 연결(Residual connection)**, **레이어 정규화(Layer normalization)** 를 사용한다. 즉, 각 서브 레이어의 출력은 LayerNorm(x + Sublayer(x))이다. 여기서 Sublayer(x)는 해당 서브 레이어에서 구현된 함수다. 

이러한 잔차 연결(Residual connection)을 쉽게 하기 위해,   
모델 내 모든 서브 레이어와 임베딩 레이어는 d_model = 512 차원의 출력을 생성한다. 

---  
  
#### 이해가 안 가는 것 : 1. 잔차 연결이란 무엇인가?   
 입력 값을 그대로 다음 층으로 전달하고, 그 입력에 서브 레이어의 출력을 더하는 구조.   
 즉, 입력을 변경하지 않고 네트워크의 출력에 더해준다.   
 정보 손실을 최소화하고 깊은 네트워크에서도 학습이 잘 되도록 할 수 있다.   
 특히 딥러닝 모델에서 층이 많아질수록 발생할 수 있는 기울기 소실 문제를 완화한다.   

 * 1.2 뭐라는거지...      
  (1) 이전 층에서 전달받은 입력 값 x가 있다.    
  (2) 이게 서브 레이어에 전달 된다.    
  (3) 서브 레이어에서 일정한 연산(feed forward network)이 이루어진다. 이 결과가 Sublayer(x)다.    
  (4) 잔차 연결이라는 건, x + sublayer(x)를 하는 것이다.    
  
*  1.2 근데 이게 왜 필요한데?    
  기울기 문제를 완화하니까.     

*  1.3 기울기 문제가 뭔데?    
  딥러닝 모델을 학습할 때, 역전파를 통해 계산된 기울기가 레이어를 거치면서 점점 작아져 파라미터가 제대로 학습되지 않는 문제. 네트워크가 깊어질 수록 소실 문제는 심각해진다.   

*  1.4 어떻게 완화하는 건데?   
  입력값 x가 그대로 다음 층으로 전달되어서 각 층이 학습해야 하는 기울기가 너무 작아지지 않기 때문. 그래서 학습이 더 빠르고 안정적으로 진행된다. 


#### 이해가 안 가는 것 : 2. 레이어 정규화란 무엇인가? (분명 수업시간에 배웠는데...)   
  각 레이어에서 입력 데이터의 분포를 정규화하는 방법. 데이터의 평균과 분산을 조정해, 학습 과정에서 변동성을 줄이고 안정성을 높인다. 이를 통해 학습 속도를 높이고 모델의 성능을 향상시킨다.
 
#### 이해가 안 가는 것 :3.  왜 LayerNorm(x + Sublayer(x))인가?
 우선 구조가 잔차 연결(입력 x와 서브 레이어에서 계산된 출력 Sublayer(x)을 더함) 하고,  그 값을 정규화 하는 구조임. 걍 이걸 식으로 쓴 거임 

#### 이해가 안 가는 것 : 4. 서브 레이어에서 함수가 구현된다는 것은 무슨 의미인가?
 해당 서브 레이어가 일정한 연산을 수행한다는 뜻. 예를 들어, Self-attention이나 Feed-forward network와 같은 특정 연산을 수행하는 함수를 서브 레이어에서 구현한다. 음... 걍 함수를 써놨다는 거네... 걍 리얼 함수가 구현되었다는 거였네... 오키
 
#### 이해가 안 가는 것 : 5. 왜 d_model = 512 차원의 출력을 생성하면 잔차 연결이 쉬워지는가? 
잔차 연결은 입력과 출력을 더하는 연산이므로, 더하려면 두 값의 차원이 일치해야 한다. d_model = 512로 모든 서브 레이어의 출력 차원을 동일하게 설정하면, 입력과 출력 간의 차원 불일치 문제가 발생하지 않기 때문에 잔차 연결을 쉽게 적용할 수 있다. 차원을 통일함으로써 각 서브 레이어가 일관성 있게 작동하고, 잔차 연결이 원활하게 이루어진다.
 
### Decorder
N = 6개의 동일한 레이어로 구성된다.    
인코더 레이어의 두 개의 서브 레이어 + 세 번째 서브 레이어로 구성되어 있다.   
이는 인코더 스택의 출력에 대해 멀티-헤드 어텐션을 수행한다. 
인코더와 마찬가지로, 각 서브 레이어 주위에 잔차 연결을 적용한 후 레이어 정규화가 이어진다.   

또한, 디코더 스택에서 Self-attention 서브 레이어를 수정하여, 각 position이 subsequent positions를 참조하지 못하도록 않도록 마스킹을 적용합니다. 출력 임베딩이 한 위치씩 오프셋됨 + 마스킹을 함 -> 위치 i의 예측은 i보다 작은 위치에서 이미 알려진 출력에만 의존하게 된다.

* 즉, 아직 현재 시점에서 안 보이는 것들(미래)을 막아주는 역할을 할 수 있는 거임. 마스킹은 출력 시퀀스의 각 토큰이 미래에 나올 값을 모르는 상태에서 결정되도록 함. 

* 그러면 각 출력은 LayerNorm(x+Masked Multi-head attention(x)) + LayerNorm(x+Multi-head attention(x)) + LayerNorm(x+FFN(x))인가?
  * 첫번째 서브레이어:Output_1 = LayerNorm(x+Masked Multi-Head Attention(x))
  * 두번째 서브레이어:Output_2 = LayerNorm( Output_1 + Multi-Head Attention(Output_1, Encoder Output)) 
  * 세번째 서브레이어:Output_3 = LayerNorm(Output_2 + FFN(Output_2))

* 오프셋... ?? : 현재 시점 i에서 i - 1의 출력을 입력으로 사용해 예측을 하니까, 디코더가 생성하는 출력 임베딩은 한 위치씩 미뤄진 오프셋 상태로 처리된다는 거임. 

# <a id="section5"></a>3.2 Attention

##  3.2 Attention

어텐션 함수는 쿼리(Query)와 일련의 키-값(Key-Value)의 집합 쌍을 출력으로 매핑하는 과정으로 설명할 수 있다. 이때 쿼리, 키, 값, 출력(query, keys, values, output)은 모두 벡터로 표현된다.   

출력은 각 값에 가중치를 곱한 값들의 가중 합(weighted sum)으로 계산된다. 이때 각 값에 할당된 가중치(weight)는 해당 값의 키와 쿼리 간의 호환성을 평가하는 함수(compatibility function)에 의해 계산된다.   

* Q = 영향을 받는 벡터 // 무엇을 찾고자 하는지를 나타내고 
* K = 영향을 주는 벡터 // 각각의 위치가 가지는 특징을 나타내고 
* V = 주는 영향의 가중치 벡터 // 각 위치에 담긴 정보를 의미함
  
<img width="173" alt="image" src="https://github.com/user-attachments/assets/2aab587c-aef3-4ec8-953b-decf900b62ab">   
<img width="344" alt="image" src="https://github.com/user-attachments/assets/a164a027-aa96-4a3a-98b4-fd9684be10f2">   

### 3.2.1 Scaled Dot-Product Attention
입력으로 쿼리(Query)와 키(Key)가 차원 d_k를 갖고, 값(Value)이 차원 d_v를 갖는다. 
(1) Key와 query의 내적(dot product)을 계산한다.  
(2) 그 각각의 값을 √(d_k)로 나눈다.  // 스케일링 하는 거임 
(3) 그 후 softmax 함수를 적용하여 값에 대한 weight를 구한다.   
식으로 보자면 Output matrix는 다음과 같이 계산된다.: 
<img width="189" alt="image" src="https://github.com/user-attachments/assets/b1c3a1f9-8812-4a82-86d1-22edfd8e2d34">


이때 가장 흔히 사용되는 두 가지 어텐션 함수는 가산 어텐션(additive attention)과 곱셈 어텐션(dot-product attention)인데,    

가산 어텐션은 단일 히든 레이어를 가진 FFN(feed-forward network)을 사용해 호환성 함수(compatibility function)를 계산한다. 곱셈 어텐션은 스케일링 인자(scaling factor: 1/√(dk))를 제외하고 동일하다.    

두 메커니즘은 이론적으로는 복잡도가 비슷하지만, 실제로는 곱셈 어텐션이 훨씬 빠르고 메모리 효율적이다. 곱셈 어텐션은 고도로 최적화된 행렬 곱셈 코드를 사용할 수 있기 때문이다.

 d_k 값이 작을 때에는 두 메커니즘의 성능이 비슷하지만, d_k값이 클 때에는 곱셈 어텐션보다 가산 어텐션이 더 좋은 성능을 보인다. d_k값이 클 때, 내적(dot product)이 매우 큰 값을 가지게 되어 소프트맥스 함수의 기울기(gradient)가 매우 작아지는 영역으로 밀려나기 때문이라 추측한다. 이러한 효과를 상쇄하기 위해, 우리는 내적에 1/√(dk)로 스케일링을 적용했다. 

---
이해가 너무 안가니까 한 번 정리하고 넘어가자...    

(1) Scaled Dot-Product Attention은 쿼리(Query), 키(Key), 값(Value)라는 세 가지 입력을 사용하여 특정한 방법으로 주어진 값들에 "주의(attention)"를 기울여, 그에 따라 중요한 정보를 추출해낸다.   

(2) 쿼리는 질문. 키는 뭔지. 값은 그래서 전달하고 싶은 정보가 뭔지. (그러니까 얼마나 중요한지 인 것 같음)... 쿼리와 키를 내적하면 이 벡터 사이의 유사도를 계산할 수 있음. 얘네끼리 얼마나 유사한지 알 수 있다는 거임. 이 정도에 따라 어떤 값(v)가 더 중요하게 사용될지(얼마나 더 많은 가중치를 줄 지)가 결정되는 거임. 

(3) 근데 여기서 문제는 쿼리와 키의 차원이 클 때임!!!! 차원이 커지면 둘 사이의 내적 값이 커지게 됨. 근데 이러면 softmax함수를 돌릴 때 유사도의 차이를 구부하기 어려워 짐   

(4) 이 문제를 해결하기 위해 스케일링을 통해 값의 크기를 줄여서 소프트맥스함수가 균형 있게 적용되어서 다양한 값에 적절한 가중치를 줄 수 있는 거임.    

(5) 그래서 결론적으로, 쿼리와 키 간의 내적 값을 구하고 > 스케일링하고 > 그 값을 소프트맥스 함수에 통과시킴 (함수는, 내적 값을 확률처럼 변환하는 역할을 함. 즉, 값들은 0과 1 사이의 숫자로 변환되는 거임. 스케일링 안 하면 다 0이고 1이고 몰려가지고 차이를 구분하기 어렵게 되는 거.) > 이 결과가 각 값에 부여될 가중치가 되는 거였삼. 

(6) 그래서 값(value)는 소프트맥스 함수로 계산된 가중치에 의해 weighted sum된다. 이게 무슨 의미냐면, 어떤 값이 쿼리와 가장 유사한 키를 갖고 있으면, 그 값이 최종 출력에 더 큰 영향을 미치는 거임. 

(7) 이런 어텐션 계산에는 가산 어텐션과 곱셈 어텐션이 있는데 이 논문에서는 곱셈 어텐션이 가산 어텐션보다 계산에서 효율적이고, 차원이 커지면 성능이 떨어지는데, 이걸 보완하기 위해 내적에 1/√(dk)로 스케일링했다는 것. 

---
### 3.2.2 Multi-Head Attention

 d_model 차원의 키, 값, 쿼리로 하나의 어텐션 함수( single attention)를 수행하는 대신,  우리는 쿼리, 키, 값에 대해, 서로 다른 학습된 선형 투영(linear projection)을 통해 각각 dk, dk, dv 차원으로 h번 선형 투영(linear projection)하는 것이 유익하다는 것을 알았다. 

 * 이게 대체 무슨 소리냐?    
: 쿼리, 키, 값 벡터들은 원래 모두 d_model 차원의 벡터들임. 근데 선형 투영을 함. 쿼리, 키, 값을 각각 W_Q, W_K, W_V라는 선형 변환 행렬을 통해 새로운 차원으로 변환한다는 뜻임. h번 했음. 즉, 원래 d_model 차원에 있던 쿼리, 키, 값 벡터를 각각 dk, dk, dv 차원으로 변환하는 것임. dk와 dv는 보통 d_model/(헤드의 수) 임. 아무튼, 더 작은 차원으로 변환되었음. 
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">

* 그 결과물이 뭔데? : h번 서로 다른 투영 행렬을 사용하면 쿼리, 키, 벡터가 h개의 각각 다른 차원의 벡터를 얻음 

 
 이렇게 투영된 쿼리, 키, 값의 각 버전에 대해 병렬로 어텐션 함수를 수행하여 dv 차원의 출력 값을 얻는다. 이 값들을 연결(concatenate)하고 다시 한 번 투영하여 최종 값을 얻게 됩니다. 이는 아래 그림을 참고하라.   
<img width="137" alt="image" src="https://github.com/user-attachments/assets/6d102322-75ac-4efd-bb58-19cfba01d28c">
 

**멀티-헤드 어텐션은 모델이 서로 다른 위치에서 다른 표현 서브스페이스로부터 정보를 동시에 참조할 수 있게 해준다. 단일 어텐션 헤드에서는 평균화가 이를 방해한다.** (우리는 그렇지 않아서 굿굿이다는 뜻임.) 식은 아래와 같다. 
<img width="266" alt="image" src="https://github.com/user-attachments/assets/b6da1403-5014-4270-84ac-b0374419188c">

참고로 투영은 파라미터 행렬로 다음과 같이 표현된다.    
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">

이 연구에서는 h = 8개의 병렬 어텐션 레이어, 즉 헤드를 사용한다.    
즉, 각 헤드에 대해 dk = dv = d_model/h = 64를 사용한다.   
각 헤드의 차원이 줄어들었기 때문에 총 계산 비용은 전체 차원의 단일 헤드 어텐션과 유사하다.   

--- 
* 정리하자면 이런 구조다.   
  - 선형 투영: 쿼리, 키, 값을 각각 서로 다른 가중치 행렬로 선형 투영하여 차원을 줄임.   
  - 멀티-헤드: 여러 번의 투영을 병렬로 진행하여 다양한 서브스페이스의 정보를 얻음.   
  - 결합: 병렬로 계산된 출력들을 연결하고, 최종적으로 다시 선형 변화나 행렬 W_o로 투영하여 출력 값을 얻음. 이때 차원은 d_model로 다시 복원되었다. 

* 그래서 최종적으로 이걸 왜 하냐?
 단일 어텐션 헤드와 다르게 멀티 헤드 어텐션은 여러 위치에서 다른 정보를 동시에 참조할 수 있다. 즉, 동시에 병렬로 처리할 수 있게 되는 것이다!! 그러니까 RNN보다 훨씬 빠르게 학습이 가능하고 더 복자한 패턴을 효과적으로 학습할 수 있다.
--- 
### 3.2.3 Applications of Attention in our Model

Transformer는 세 가지 방식으로 multi-head attention을 사용한다.   

1. 인코더-디코더 어텐션 레이어(Encoder-Decoder Attention layer) 에서   
쿼리(Query)가 이전 디코더 레이어에서 얻고.   
메모리 키(Key)와 값(Value)은 인코더의 output에서 얻는다.   
(그래서 decoder의 각 position이 input sequence의 모든 position를 참조할 수 잇다.)   
이는 sequence-to-sequence 모델에서 일반적으로 사용하는 인코더-디코더 어텐션 메커니즘과 같다.

2. 인코더의 self-attention 레이어에서
모든 키, 값, 쿼가 같은 위치에서 온다.
인코더의 이전 레이어 output에서 키, 값, 쿼리를 얻은 것이다.
인코더의 각 position은 이전 layer의 모든 position에 참조할 ㅏ수 있다.

3. 디코더의 self-attention 레이어에서
디코더의 각 위치가 디코더 내의 모든 위치, 그 중에서도 현재 위치까지에 대해 참조할 수 있게 한다.
디코더에서는 auto-regressive 특성을 유지하기 위해 왼쪽(미래) 방향으로의 정보 흐름을 막아야 한다. (마스킹으로 미래 시점의 단어를 막는다는 의미) 이를 위해, scaled dot-product attention에서 소프트맥스의 입력 중 잘못된 연결(미래와의 연결)에 해당하는 모든 값을 -∞로 설정하여 마스킹을 적용한다. 

# <a id="section6"></a>3.3 Position-wise Feed-Forward Networks
attention sub-layer 외에도, encoder와 decoder의 각 layer에는 완전 연결된 FFN(fully connected feed-forward network)을 포함하고 있으며, 이는 각 position에 대해 독립적으로 동일하게 적용된다. 이 네트워크는 두 개의 선형 변환과 그 사이에 ReLU 활성화 함수를 갖고 있다.

> FFN(x) = max(0, xW1 + b1)W2 + b2 (2)

각 position에서의 선형 변환은 동일하지만, 레이어 간에는 서로 다른 parameters를 사용합니다. 이건 커널 크기가 1인 두 개의 컨볼루션(convolution)이라고도 볼 수 있다. 입력과 출력의 차원은 d_model = 512이며, 내부 레이어의 차원은 d_ff = 2048이다.

# <a id="section7"></a>3.4 Embeddings and Softmax
토큰과 출력 토큰을 차원 d_model의 벡터로 변환한다. 우리는 또한 디코더 출력을 예측된 다음 토큰 확률로 변환하기 위해 일반적으로 학습된 선형 변환과 소프트맥스 함수를 사용한다. 우리 모델에서는 두 개의 임베딩 레이어와 프리 소프트맥스 선형 변환 간에 동일한 가중치 행렬을 공유한다. 임베딩 레이어에서는 이러한 가중치를 √d_model로 곱합니다. (위에서 한 번씩 다 정리 된 내용)

<img width="370" alt="image" src="https://github.com/user-attachments/assets/0860687a-9fad-4d38-b7cc-a2a80875eb7c">

표 1: 서로 다른 레이어 유형에 대한 최대 경로 길이, 레이어별 복잡도 및 최소 순차 연산 수. n은 시퀀스 길이, d는 표현 차원, k는 컨볼루션의 커널 크기, r은 제한된 자기 주의(restricted self-attention)에서 이웃의 크기

# <a id="section8"></a>3.5 Positional Encoding
트랜스포머는 순환 구조나 컨볼루션이 없기 때문에, (그래서 시퀀스의 위치 정보가 없음), 모델이 시퀀스의 순서를 활용할 수 있도록 토큰의 상대적 또는 절대적인 위치에 대한 정보를 주입한다.  

이를 위해 인코더와 디코더 스택의 하단에 입력 임베딩에 위치 인코딩(positional encodings)을 추가한다. 위치 인코딩은 임베딩과 동일한 차원 d_model을 가지므로 두 값을 더할 수 있다. 위치 인코딩에는 학습된 것과 고정된 것 등 다양한 선택이 있다.

서로 다른 주파수의 sin, cos 함수를 사용한다:

PE(pos, 2i) = sin(pos / 10000^(2i / d_model))
PE(pos, 2i + 1) = cos(pos / 10000^(2i / d_model))

pos = 위치, i = 차원이다. 즉, 위치 인코딩의 각 차원은 사인 함수에 대응한다. 파장(geometric progression wavelengths)은 2π에서 10000 · 2π까지 기하급수적으로 증가한다.   

이 함수를 선택한 이유는 고정된 오프셋 k에 대해 PE_pos+k를 PE_pos의 선형 함수로 나타낼 수 있기 때문이다. 선형 함수로 나타내면, 모델이 상대적 위치에 따라 주의를 기울이는 것을 쉽게 배울 수 있을 것이라고 가정했다. 

대신 학습된 위치 임베딩을 사용하는 실험을 했는데, 두 가지 버전이 거의 동일한 결과를 생성한다는 것을 발견했다. 사인파 버전을 선택했는데, 이는 모델이 훈련 중 접한 시퀀스 길이보다 긴 시퀀스 길이에도 추론 할 수 있기 때문이다. 


# <a id="section9"></a>4.  WhySelf-Attention
self-attention 레이어의 다양한 측면을  recurrent and convolution layers 레이어와 비교합니다.
이때 recurrent and convolution layers는 일반적으로 변수 길이의 기호 표현 시퀀스(symbol representations) (x1,..., xn)를 다른 동일 길이의 시퀀스 (z1,...,zn)로 매핑하는 데 사용되는 레이어임. 여기서 xi, zi는 Rd에 속하는 값이다. 이는 일반적인 시퀀스 변환 인코더 또는 디코더의 숨겨진(hidden) 레이어와 같은 것이다. 

self-attention 를 제안하는 이유는 3가지다.    

1. 레이어당 총 계산 복잡성   

2. 최소한의 순차 연산 수로 측정되는 병렬화 가능한 계산의 양    
  
3. 네트워크 내에서 장기 의존성(long-range dependencies) 사이의 경로 길이.
장기 의존성을 학습하는 것은 많은 시퀀스 변환 작업에서 주요 도전 과제다. 이러한 의존성을 학습하는 능력에 영향을 미치는 주요 요소 중 하나는 **네트워크에서 신호가 앞뒤로 이동해야 하는 경로의 길이**다. 입력 및 출력 시퀀스의 임의의 위치 조합 간의 경로가 짧을수록 장기 의존성을 학습하기가 더 쉬워진다. 따라서 다양한 레이어 유형으로 구성된 네트워크에서 입력 및 출력의 두 위치 간 최대 경로 길이도 비교했다.

비교한 결과를 표에서 확인할 수 있다: 
<img width="370" alt="image" src="https://github.com/user-attachments/assets/20101274-aad5-40fd-8afa-db1bd1ba786c">
* 시퀀스 길이 n, 표현 차원 d 

실험결과를 통해 제안하는 이유의 근거를 아라보자.     

1. 계산 복잡성 측면에서   
self-attention 레이어는 n < d일때 (대개 기계 번역의 최첨단 모델에서 사용되는 문장 표현의 경우에) 더 빠르다. 그리고 매우 긴 시퀀스가 포함된 작업은 계산 성능을 개선하기 위해, self-attention을 해당 출력 위치를 중심으로 입력 시퀀스 내의 크기 r인 이웃만 고려하도록 제한할 수 있다. 이렇게 하면 최대 경로 길이가 O(n/r)로 증가한다. 
   
3. 병렬화.   
self-attention 레이어는 모든 위치를 일정한 수의 순차적으로 실행된 연산으로 연결한다.   
그러나 순환 레이어는 O(n) 순차 연산을 필요로 한다.    

3. 길이
커널 너비 k < n인 단일 컨볼루션 레이어는 모든 입력 및 출력 위치 쌍을 연결하지 않는다. 이를 위해서는 연속 커널의 경우 O(n/k) 개의 컨볼루션 레이어가 필요하거나, 팽창 컨볼루션의 경우 O(logk(n))가 필요하여 네트워크 내의 임의의 두 위치 간의 가장 긴 경로의 길이가 증가한다. 컨볼루션 레이어는 일반적으로 순환 레이어보다 k배 더 비싸다. 그러나 분리 가능한 컨볼루션(separable convolutions)은 복잡성을 상당히 줄여 O(k · n · d + n · d²)로 만든다. 그러나 k = n인 경우에도 분리 가능한 컨볼루션의 복잡성은 self-attention 레이어와 포인트-와이즈(feed-forward) 레이어의 조합과 동일하며, 이는 우리가 모델에서 채택한 접근 방식이다. 

추가로, 
self-attention은 더 해석 가능한 모델을 제공할 수 있다. 
우리는 모델의 주의(attention) 분포를 검사하고 부록에서 예제를 제시하고 논의한다. 개별 attention 헤드가 분명히 서로 다른 작업을 수행하도록 학습할 뿐만 아니라, 많은 헤드가 문장의 구문 및 의미 구조와 관련된 행동을 나타내는 것처럼 보인다.


//////// 나중에...^_^
# <a id="section10"></a>5.  Training
## 5.1 Training Data and Batching
## 5.2 Hardware and Schedule
## 5.3 Optimizer
## 5.4 Regularization

# <a id="section11"></a>6.  Results
##  6.1 MachineTranslation
##  6.2 ModelVariations
##  6.3 EnglishConstituencyParsing

# <a id="section12"></a>7.  Conclusion

# <a id="section13"></a>8.  정리 

