---
title:  "10월/ 1. 트랜스포머 공부하기(1) - Attention Is All You Need 논문 읽기(1)  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section0">0. Abstract.  </a></li>   
  <li><a href="#section1">1. Introduction  </a></li>   
  <li><a href="#section2">2. Background  </a></li>
  <li><a href="#section3">3. ModelArchitecture </a></li>
  <li><a href="#section4">3.1 ModelArchitecture/Encoder and Decoder Stacks </a></li>
</ul>

-------------------------------------------------------------------   

# 0. 
[Attention Is All You Need 논문링크](https://arxiv.org/pdf/1706.03762)
가능한 원문 그대로 읽어서 기록흠   
모르거나 헷갈리는 개념이 나오면 **볼드**처리 후 각 단락마다 맨 밑에 설명을 작성함 설명하면서 작성함    
* 중간중간 이렇게 처리한 건 잘 이해가 안 되는 부분 정리해서 첨삭한 내용임 


# <a id="section0"></a>0.  Abstract. 

  현재 많이 쓰이는 **시퀀스 변환 모델**은 복잡한 **순환 신경망**(Recurrent Neural Networks, RNN) 또는 **합성곱 신경망**(Convolutional Neural Networks, CNN)을 기반으로 하며,  **인코더**와 **디코더**를 포함하고 있다. 가장 성능이 좋은 모델들은 인코더와 디코더를 **어텐션 메커니즘**을 통해 연결한다. 

이 논문에서는 이러한 복잡성을 없애고 순수하게 어텐션 메커니즘만을 기반으로 한 새로운 간단한 네트워크 아키텍처인 트랜스포머(Transformer)를 제안한다. 이 모델은 순환(RNN)이나 합성곱(CNN)을 완전히 배제한다. 두 가지 **기계 번역 작업**에 대한 실험 결과, 이 모델(트랜스포머)이 기존의 모델들보다 더 우수한 품질을 보이며, **병렬화 가능성**이 더 높고 훈련 시간이 현저히 적게 소요된다는 것을 확인했다. 

이 논문에 실린 트랜스포머 모델은 2014년 WMT 영어-독일어 번역 작업에서 28.4 BLEU 점수를 기록했으며, 이는 기존 최고 성능을 보였던 모델들(앙상블 포함)보다 2 **BLEU** 이상 개선된 결과이다. 또한, 2014년 WMT 영어-프랑스어 번역 작업에서는 41.8 BLEU로 단일 모델 기준 새로운 최고 성능을 기록했으며, 8개의 GPU에서 3.5일 동안 훈련한 결과, 기존 문헌에 나와 있는 최고 성능 모델들보다 훈련 비용이 매우 적었다. 우리는 트랜스포머가 영어 구문 분석(constituency parsing)에도 잘 적용된다는 것을 확인했으며, 대규모 데이터셋과 제한된 데이터셋 모두에서 성공적인 결과를 보였다. 

-------------- 
* 시퀀스 변환 모델: 입력 시퀀스를 받아서 출력 시퀀스로 변환하는 모델. 예를 들어, 기계 번역에서 입력된 문장을 다른 언어로 변환하는 작업이 이에 해당한다. 자연어 처리(NLP)에서 자주 사용되며, 번역뿐 아니라 음성 인식, 요약, 텍스트 생성 같은 작업에 활용된다.   

* 순환 신경망(RNN): 순차적 데이터(시퀀스)를 처리하기 위해 설계된 신경망. 시퀀스의 각 단계에서 이전 단계의 출력을 다음 단계로 전달해, 이전 정보를 기억하고 현재 입력을 처리할 수 있다. 주로 자연어 처리에서 사용되며, 텍스트 생성, 번역, 음성 인식 등에 활용된다. 시퀀스가 길어지면 정보가 소실되거나 장기 의존 관계(long-term dependencies)를 학습하기 어려운 단점이 있다.   

* 합성곱 신경망(CNN): 공간적 데이터(이미지)를 처리하는 데 주로 사용되며, 특징 추출에 탁월한 성능을 보입니다. 입력 데이터를 여러 필터를 통해 합성곱(Convolution) 연산을 적용하여 중요한 특징을 추출하고, 이를 통해 다양한 작업을 수행한다. 최근에는 텍스트 처리 작업에도 CNN이 활용되고 있다. 주로 이미지 인식 및 영상 처리에 사용되지만, NLP에서도 합성곱을 통해 문장의 패턴을 추출하는 방식으로 응용된다.   

* 인코더: 입력 데이터를 벡터로 변환하는 네트워크. NLP 작업에서 인코더는 입력 시퀀스를 처리하여, 그 의미를 함축한 벡터 표현을 생성한다. 예를 들어, 문장을 하나의 고차원 벡터로 변환하는 역할을 한다.   
 
* 디코더: 인코더에서 생성된 압축된 벡터 표현을 입력받아, 이를 바탕으로 출력 시퀀스를 생성하는 역할. 주로 번역 작업에서, 인코더가 입력된 문장을 벡터로 변환한 후, 디코더가 해당 벡터를 사용하여 목표 언어로 변환된 문장을 출력한다.
 
 * 어텐션 매커니즘: 입력 시퀀스의 각 요소를 처리할 때 다른 중요한 요소에 더 많은 가중치를 부여하여 처리하는 기법. 특히, 문장 내에서 중요한 단어에 집중하여 그 관계를 더 잘 파악할 수 있도록 도와준다. 트랜스포머에서 어텐션 매커니즘ㅁ에 집중한 이유는, RNN 등의 모델은 시퀀스가 길어지면 정보를 잊는 문제가 있는데, 어텐션 메커니즘은 모든 입력 요소를 한 번에 살펴보며 중요한 정보를 놓치지 않게 한다. ... 논문 더 읽다보면 자세히 나옴 

* 기계 번역 작업: 한 언어의 텍스트를 다른 언어로 자동으로 변환하는 작업. NLP의 대표적인 응용 분야로, 딥러닝과 트랜스포머 모델은 기계 번역의 성능을 크게 향상시켰다. 예시로, 영어에서 독일어로 문장을 번역하는 작업이 있다.   
   
* 병렬화 가능성: 모델이 여러 작업을 동시에 처리할 수 있는 능력. RNN은 순차적으로 데이터를 처리하기 때문에 병렬화가 어렵지만, 트랜스포머는 병렬로 데이터를 처리할 수 있어, 더 빠른 훈련 속도를 가잔다. 병렬화가 잘 이루어지면 훈련 속도가 크게 향상되며, 대규모 데이터를 더 효율적으로 처리할 수 있다.

* BLEU: 기계 번역의 품질을 평가하기 위해 사용되는 지표. 번역된 문장이 정답 문장과 얼마나 유사한지를 평가하는데, 주로 단어 간의 유사성과 정확도를 측정한다. BLEU 점수는 0에서 100까지의 범위를 가지며, 점수가 높을수록 번역된 문장이 원문에 가깝다는 것을 의미한다.


# <a id="section1"></a>1.  Introduction

순환 신경망(Recurrent Neural Networks, RNN), 장단기 기억(Long Short-Term Memory, LSTM), 게이트 순환 신경망(Gated Recurrent Neural Networks, GRU)은 언어 모델링 및 기계 번역과 같은 sequence modeling 및 transduction 문제애서 sota로 자리를 잡았다. 이후 많은 연구들이 순환 언어 모델과 인코더-디코더 아키텍처의 경계를 확장하기 위해 지속적으로 노력해 왔다.   

순환 모델(Recurrent model)은 일반적으로 input 및 output sequence의 각 위치에 따라 계산을 분리하여 처리한다. 시퀀스의 각 위치(symbol positions)를 steps와 위치에 따라 정렬하여, 이전 은닉 상태(hidden state )\( h_{t-1}\)와 position t가 input인 hidden state \( h_t \)를 생성한다.    

* 순차적인 계산에 대해 풀어서 설명하자면: 순환 신경망(RNN)과 같은 모델은 입력 시퀀스의 각 단어(또는 토큰)를 시간 순서대로 처리한다. 즉, 시퀀스의 각 위치(예: t번째 단어)를 차례로 처리하며, 이전 시점의 계산 결과(은닉 상태)를 기억한 상태에서 현재 시점의 입력을 반영해 계산을 수행함. 즉슨 시퀀스의 t번째 위치에서 출력이 결정될 때, t-1번째 위치의 정보(은닉 상태)와 현재 t번째 입력이 모두 반영됨.   

이러한 순차적 처리 특성(ht-1 처리 후에야 ht를 처리할 수 있는 순차적인 특성)은 병렬 처리를 어렵게 만드는데, 이는 시퀀스 길이가 길어질수록 메모리 제한으로 인해 여러 예제를 배치(batch)로 처리하는 데 한계를 겪게 된다. 최근 연구는 계산 효율성을 개선하기 위해 인수분해 기법과 조건부 계산을 사용하여 큰 발전을 이루었으며, 후자는 모델 성능도 향상시켰다. 그러나 순차적 계산의 근본적인 제약은 여전히 남아 있다.   

* 병렬화가 어려워지는 이유: 이전 시점의 계산이 끝나야 다음 시점의 계산을 할 수 있기 때문에 병렬로 처리하기 어렵다는 단점이 있다. 즉, 시퀀스의 각 요소가 독립적으로 동시에 처리되지 않고, 하나씩 차례로 계산되어야 한다는 것을 의미함. 요약하자면 각 시퀀스 위치에서의 계산이 순차적으로 일어나며 이전 결과에 의존한다는 단점이 있다는 것이다. 

어텐션 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링과 변환 모델의 필수적인 부분이 되었다. 이는 입력 또는 출력 시퀀스에서 거리와 상관없이 의존성을 모델링할 수 있게 해준다. 하지만 일부 경우를 제외하고, 이러한 어텐션 메커니즘은 대부분 순환 신경망과 함께 사용된다. 

* 아무튼 순환 신경망이랑 섞어 쓰니까 효율적인 병렬화가 불가능하다는 것을 의미함 

본 연구에서는 순환 구조(recurrence)를 제거하고 전적으로 어텐션 메커니즘에 의존하여, 입력과 출력 간의 전역 의존성을 모델링하는 트랜스포머(Transformer)를 제안한다. 

트랜스포머는 병렬화 가능성을 크게 향상시켰으며, 8개의 P100 GPU에서 12시간 만에 훈련하여 번역 품질에서 새로운 최고 성능에 도달할 수 있었다.

* 참고로, cpu와 gpu로 본 병렬화 차이를 시각적으로 이해시켜주는 [영상](https://www.youtube.com/watch?v=1BAZf3PsjWA0) ... rnn은 병렬화가 안되니까 gpu를 잘 활용을 못함 ㅠㅠ 

# <a id="section2"></a>2. Background

순차적인 계산을 줄이려는 모델(Extended Neural GPU, ByteNet, ConvS2S 등)이 있었다. 이들 모두 합성곱 신경망 CNN(Convolutional Neural Networks)을 기본 구성 요소로 사용하여, 입력 및 출력의 모든 위치에 대해 병렬로  hidden representations를 계산한다. 이러한 모델들에서, 임의의 두 입력 또는 출력 위치 간의 신호를 연결하는 데 필요한 연산 수는 그 위치 간의 거리에 따라 증가한다. ConvS2S의 경우에는 선형적으로, ByteNet은 로그 형태로 증가한다. 이는 멀리 떨어진 위치 간의 의존성을 학습하는 것을 더 어렵게 만든다.   

트랜스포머에서는 이를 상수 개수의 연산으로 줄였지만, 어텐션 가중치가 적용된 위치의 평균으로 인해 해상도가 감소하는 문제가 발생할 수 있다. 이 문제는 3.2절에서 설명하는 멀티-헤드 어텐션(Multi-Head Attention)으로 보완된다.   
Self-attention(intra-attention)은 단일 시퀀스의 다양한 위치 간의 관계를 계산하여 그 시퀀스의 표현을 생성하는 어텐션 메커니즘입니다. Self-attention은 독해(Reading Comprehension), 요약(Abstractive Summarization), 텍스트 함의(Textual Entailment), 작업 독립적 문장 표현 학습 등 다양한 작업에서 성공적으로 사용되었다.

End-to-end 메모리 네트워크는 sequence aligned recurrence(시퀀스 정렬 순환) 대신에 순환 어텐션 메커니즘(recurrent attention mechanism)을 기반으로 하며, 간단한 언어 질문 응답(single-language QA) 및 언어 모델링 작업에서 좋은 성능을 보인다.

한편, 트랜스포머는 RNN이나 CNN을 사용하지 않고 완전히 Self-attention만을 사용하여 입력과 출력의 표현을 계산하는 최초의 변환(transduction) 모델이다. 

다음 섹션에서는 트랜스포머에 대해 설명하고, Self-attention을 어떻게 동작하게 하고 기존의 모델들보다 가지는 장점에 대해 논의할 것이다.   



# <a id="section3"></a>3.  ModelArchitecture

대부분의 경쟁력 있는 신경망 기반 시퀀스 변환 모델은 **인코더-디코더** 구조를 가지고 있다.   

인코더는 입력 시퀀스인 (x1,...,xn)의 기호 표현을 연속된 표현 시퀀스 z = (z1,...,zn)로 변환한다.  
디코더는 z가 주어지면 하나씩 출력 시퀀스 (y1,...,ym)를 생성한다.   

이러한 인코더-디코더 모델은 각 단계에서 자동회귀적(auto-regressive)으로 동작하며, 이전에 생성된 output을 다음 output을 생성할 때 추가 입력으로 사용한다. 

* 여기서 트랜스포머 이전 인코더-디코더 구조의 모델들이 이전 단계가 완료된 후에 다음단계를 수행할 수 있는, 즉 병렬적으로 처리가 불가능한 구조였음을 알 수 있다. 

트랜스포머는 이 전체적인 구조를 따라간다.   
인코더와 디코더에서 **스택으로 쌓인 self attention**과 **위치별로 완전히 연결된 계층(point-wise, fully connected layers)**을 사용하면서.

* 이게 무슨 뜻이냐면... 아직 이해가 잘 안 되기는 하는데... 트랜스포머 모델은 인코더와 디코더의 기본 구조를 유지하고 있음. 그리고, point-wise, fully connected layers 즉 위치별로 입력을 각각 독립적으로 처리하는데 또 완전히 연결된 구조(?)의 계층이 있음. 이게 층층이 쌓여 있음. self attention이 각 계층이 다른 계층들과 상호작용하면서 관게를 계산하고, 가중치 부여하고, 의존성을 모델링함. 얘네가 쌓아올려져있는 형태로 아키텍쳐가 구성되어있다는 뜻임. 사진을 보면 더 잘 이해할 수 있음.

* 근데 사진에 논문에서는 설명 안해주는 positional Embeeing, input Embedding 등 세미나에서 배우기도 했던 놈들이 있음....자세히 알아야 됨...그래서 얘네들은 [트랜스포머의 종류, 어텐션 자세히 공부하기, 트랜스포머의 구조, 트랜스포머 모델들] 등등 다른 것들과 묶어서 [다음 포스트](https://ctruss119.github.io/study/gyul7/)에서 추가로 작성할 것임... 


# <a id="section4"></a>3.1  Encoder and Decoder Stacks

### Encorder
N = 6개의 동일한 레이어로 구성된다.   
각 레이어는 두 개의 서브 레이어를 갖고 있다.   
첫 번째: multi-head self-attention 메커니즘,   
두 번째: 간단한, 위치별, 완전히 연결된 feed-forward network.   

각 서브 레이어는 **잔차 연결(Residual connection)**, **레이어 정규화(Layer normalization)** 를 사용한다. 즉, 각 서브 레이어의 출력은 LayerNorm(x + Sublayer(x))이다. 여기서 Sublayer(x)는 해당 서브 레이어에서 구현된 함수다. 

이러한 잔차 연결(Residual connection)을 쉽게 하기 위해,   
모델 내 모든 서브 레이어와 임베딩 레이어는 d_model = 512 차원의 출력을 생성한다. 

---  
  
#### 이해가 안 가는 것 : 1. 잔차 연결이란 무엇인가?   
 입력 값을 그대로 다음 층으로 전달하고, 그 입력에 서브 레이어의 출력을 더하는 구조.   
 즉, 입력을 변경하지 않고 네트워크의 출력에 더해준다.   
 정보 손실을 최소화하고 깊은 네트워크에서도 학습이 잘 되도록 할 수 있다.   
 특히 딥러닝 모델에서 층이 많아질수록 발생할 수 있는 기울기 소실 문제를 완화한다.   

 * 1.2 뭐라는거지...      
  (1) 이전 층에서 전달받은 입력 값 x가 있다.    
  (2) 이게 서브 레이어에 전달 된다.    
  (3) 서브 레이어에서 일정한 연산(feed forward network)이 이루어진다. 이 결과가 Sublayer(x)다.    
  (4) 잔차 연결이라는 건, x + sublayer(x)를 하는 것이다.    
  
*  1.2 근데 이게 왜 필요한데?    
  기울기 문제를 완화하니까.     

*  1.3 기울기 문제가 뭔데?    
  딥러닝 모델을 학습할 때, 역전파를 통해 계산된 기울기가 레이어를 거치면서 점점 작아져 파라미터가 제대로 학습되지 않는 문제. 네트워크가 깊어질 수록 소실 문제는 심각해진다.   

*  1.4 어떻게 완화하는 건데?   
  입력값 x가 그대로 다음 층으로 전달되어서 각 층이 학습해야 하는 기울기가 너무 작아지지 않기 때문. 그래서 학습이 더 빠르고 안정적으로 진행된다. 


#### 이해가 안 가는 것 : 2. 레이어 정규화란 무엇인가? (분명 수업시간에 배웠는데...)   
  각 레이어에서 입력 데이터의 분포를 정규화하는 방법. 데이터의 평균과 분산을 조정해, 학습 과정에서 변동성을 줄이고 안정성을 높인다. 이를 통해 학습 속도를 높이고 모델의 성능을 향상시킨다.
 
#### 이해가 안 가는 것 :3.  왜 LayerNorm(x + Sublayer(x))인가?
 우선 구조가 잔차 연결(입력 x와 서브 레이어에서 계산된 출력 Sublayer(x)을 더함) 하고,  그 값을 정규화 하는 구조임. 걍 이걸 식으로 쓴 거임 

#### 이해가 안 가는 것 : 4. 서브 레이어에서 함수가 구현된다는 것은 무슨 의미인가?
 해당 서브 레이어가 일정한 연산을 수행한다는 뜻. 예를 들어, Self-attention이나 Feed-forward network와 같은 특정 연산을 수행하는 함수를 서브 레이어에서 구현한다. 음... 걍 함수를 써놨다는 거네... 걍 리얼 함수가 구현되었다는 거였네... 오키
 
#### 이해가 안 가는 것 : 5. 왜 d_model = 512 차원의 출력을 생성하면 잔차 연결이 쉬워지는가? 
잔차 연결은 입력과 출력을 더하는 연산이므로, 더하려면 두 값의 차원이 일치해야 한다. d_model = 512로 모든 서브 레이어의 출력 차원을 동일하게 설정하면, 입력과 출력 간의 차원 불일치 문제가 발생하지 않기 때문에 잔차 연결을 쉽게 적용할 수 있다. 차원을 통일함으로써 각 서브 레이어가 일관성 있게 작동하고, 잔차 연결이 원활하게 이루어진다.
 
### Decorder
N = 6개의 동일한 레이어로 구성된다.    
인코더 레이어의 두 개의 서브 레이어 + 세 번째 서브 레이어로 구성되어 있다.   

이는 인코더 스택의 출력에 대해 멀티-헤드 어텐션을 수행한다. 
인코더와 마찬가지로, 각 서브 레이어 주위에 잔차 연결을 적용한 후 레이어 정규화가 이어진다.   

또한, 디코더 스택에서 Self-attention 서브 레이어를 수정하여, 각 position이 subsequent positions를 참조하지 못하도록 않도록 마스킹을 적용합니다.   

이러한 마스킹과 출력 임베딩이 한 위치씩 오프셋되는 방식이 결합되어, 위치 i의 예측은 i보다 작은 위치에서 이미 알려진 출력에만 의존하게 됩니다.

* 그러면 각 출력은 LayerNorm(x+Masked Multi-head attention(x)) + LayerNorm(x+Multi-head attention(x)) + LayerNorm(x+FFN(x))인가?
* Masked Multi-head attention(x)가 잘 이해가 안 간다... 왜? 미래 시점을 막아주는 역할이라고? 

------------
생각보다 너무 길어져서 한 번 자르고 가는걸로... 
