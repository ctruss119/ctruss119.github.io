---
title:  "10월/ 1. 트랜스포머 공부하기(1) - Attention Is All You Need 논문 읽기(2)  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">3.2 ModelArchitecture/Attention </a></li>
  <li><a href="#section2">3.3 ModelArchitecture/Position-wise Feed-Forward Networks </a></li>
  <li><a href="#section3">3.4 ModelArchitecture/Embeddings and Softmax </a></li>
  <li><a href="#section4">3.5 ModelArchitecture/Positional Encoding </a></li>
  <li><a href="#section5">4.  WhySelf-Attention </a></li>
  <li><a href="#section6">5.  Training </a></li>
  <li><a href="#section7">6.  Results </a></li>
  <li><a href="#section8">7.  Conclusion </a></li>
  <li><a href="#section9">8.  정리 </a></li>  
</ul>

-------------------------------------------------------------------   
생각보다 길어져서 한 번 자르고 가는 걸로... 
---
# <a id="section1"></a>3.2 ModelArchitecture/Attention

##  3.2 Attention

어텐션 함수는 쿼리(Query)와 일련의 키-값(Key-Value)의 집합 쌍을 출력으로 매핑하는 과정으로 설명할 수 있다. 이때 쿼리, 키, 값, 출력(query, keys, values, output)은 모두 벡터로 표현된다.   

출력은 각 값에 가중치를 곱한 값들의 가중 합(weighted sum)으로 계산된다. 이때 각 값에 할당된 가중치(weight)는 해당 값의 키와 쿼리 간의 호환성을 평가하는 함수(compatibility function)에 의해 계산된다.   

* Q = 영향을 받는 벡터 // 무엇을 찾고자 하는지를 나타내고 
* K = 영향을 주는 벡터 // 각각의 위치가 가지는 특징을 나타내고 
* V = 주는 영향의 가중치 벡터 // 각 위치에 담긴 정보를 의미함
  
<img width="173" alt="image" src="https://github.com/user-attachments/assets/2aab587c-aef3-4ec8-953b-decf900b62ab">   
<img width="344" alt="image" src="https://github.com/user-attachments/assets/a164a027-aa96-4a3a-98b4-fd9684be10f2">   

### 3.2.1 Scaled Dot-Product Attention
입력으로 쿼리(Query)와 키(Key)가 차원 d_k를 갖고, 값(Value)이 차원 d_v를 갖는다. 
(1) Key와 query의 내적(dot product)을 계산한다.  
(2) 그 각각의 값을 √(d_k)로 나눈다.  // 스케일링 하는 거임 
(3) 그 후 softmax 함수를 적용하여 값에 대한 weight를 구한다.   
식으로 보자면 Output matrix는 다음과 같이 계산된다.: 
<img width="189" alt="image" src="https://github.com/user-attachments/assets/b1c3a1f9-8812-4a82-86d1-22edfd8e2d34">


이때 가장 흔히 사용되는 두 가지 어텐션 함수는 가산 어텐션(additive attention)과 곱셈 어텐션(dot-product attention)인데,    

가산 어텐션은 단일 히든 레이어를 가진 FFN(feed-forward network)을 사용해 호환성 함수(compatibility function)를 계산한다. 곱셈 어텐션은 스케일링 인자(scaling factor: 1/√(dk))를 제외하고 동일하다.    

두 메커니즘은 이론적으로는 복잡도가 비슷하지만, 실제로는 곱셈 어텐션이 훨씬 빠르고 메모리 효율적이다. 곱셈 어텐션은 고도로 최적화된 행렬 곱셈 코드를 사용할 수 있기 때문이다.

 d_k 값이 작을 때에는 두 메커니즘의 성능이 비슷하지만, d_k값이 클 때에는 곱셈 어텐션보다 가산 어텐션이 더 좋은 성능을 보인다. d_k값이 클 때, 내적(dot product)이 매우 큰 값을 가지게 되어 소프트맥스 함수의 기울기(gradient)가 매우 작아지는 영역으로 밀려나기 때문이라 추측한다. 이러한 효과를 상쇄하기 위해, 우리는 내적에 1/√(dk)로 스케일링을 적용했다. 

---
이해가 너무 안가니까 한 번 정리하고 넘어가자...    

(1) Scaled Dot-Product Attention은 쿼리(Query), 키(Key), 값(Value)라는 세 가지 입력을 사용하여 특정한 방법으로 주어진 값들에 "주의(attention)"를 기울여, 그에 따라 중요한 정보를 추출해낸다.   

(2) 쿼리는 질문. 키는 뭔지. 값은 그래서 전달하고 싶은 정보가 뭔지. (그러니까 얼마나 중요한지 인 것 같음)... 쿼리와 키를 내적하면 이 벡터 사이의 유사도를 계산할 수 있음. 얘네끼리 얼마나 유사한지 알 수 있다는 거임. 이 정도에 따라 어떤 값(v)가 더 중요하게 사용될지(얼마나 더 많은 가중치를 줄 지)가 결정되는 거임. 

(3) 근데 여기서 문제는 쿼리와 키의 차원이 클 때임!!!! 차원이 커지면 둘 사이의 내적 값이 커지게 됨. 근데 이러면 softmax함수를 돌릴 때 유사도의 차이를 구부하기 어려워 짐   

(4) 이 문제를 해결하기 위해 스케일링을 통해 값의 크기를 줄여서 소프트맥스함수가 균형 있게 적용되어서 다양한 값에 적절한 가중치를 줄 수 있는 거임.    

(5) 그래서 결론적으로, 쿼리와 키 간의 내적 값을 구하고 > 스케일링하고 > 그 값을 소프트맥스 함수에 통과시킴 (함수는, 내적 값을 확률처럼 변환하는 역할을 함. 즉, 값들은 0과 1 사이의 숫자로 변환되는 거임. 스케일링 안 하면 다 0이고 1이고 몰려가지고 차이를 구분하기 어렵게 되는 거.) > 이 결과가 각 값에 부여될 가중치가 되는 거였삼. 

(6) 그래서 값(value)는 소프트맥스 함수로 계산된 가중치에 의해 weighted sum된다. 이게 무슨 의미냐면, 어떤 값이 쿼리와 가장 유사한 키를 갖고 있으면, 그 값이 최종 출력에 더 큰 영향을 미치는 거임. 

(7) 이런 어텐션 계산에는 가산 어텐션과 곱셈 어텐션이 있는데 이 논문에서는 곱셈 어텐션이 가산 어텐션보다 계산에서 효율적이고, 차원이 커지면 성능이 떨어지는데, 이걸 보완하기 위해 내적에 1/√(dk)로 스케일링했다는 것. 

---
### 3.2.2 Multi-Head Attention

 d_model 차원의 키, 값, 쿼리로 하나의 어텐션 함수( single attention)를 수행하는 대신,  우리는 쿼리, 키, 값에 대해, 서로 다른 학습된 선형 투영(linear projection)을 통해 각각 dk, dk, dv 차원으로 h번 선형 투영(linear projection)하는 것이 유익하다는 것을 알았다. 

 * 이게 대체 무슨 소리냐?    
: 쿼리, 키, 값 벡터들은 원래 모두 d_model 차원의 벡터들임. 근데 선형 투영을 함. 쿼리, 키, 값을 각각 W_Q, W_K, W_V라는 선형 변환 행렬을 통해 새로운 차원으로 변환한다는 뜻임. h번 했음. 즉, 원래 d_model 차원에 있던 쿼리, 키, 값 벡터를 각각 dk, dk, dv 차원으로 변환하는 것임. dk와 dv는 보통 d_model/(헤드의 수) 임. 아무튼, 더 작은 차원으로 변환되었음. 
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">

* 그 결과물이 뭔데? : h번 서로 다른 투영 행렬을 사용하면 쿼리, 키, 벡터가 h개의 각각 다른 차원의 벡터를 얻음 

 
 이렇게 투영된 쿼리, 키, 값의 각 버전에 대해 병렬로 어텐션 함수를 수행하여 dv 차원의 출력 값을 얻는다. 이 값들을 연결(concatenate)하고 다시 한 번 투영하여 최종 값을 얻게 됩니다. 이는 아래 그림을 참고하라.   
<img width="137" alt="image" src="https://github.com/user-attachments/assets/6d102322-75ac-4efd-bb58-19cfba01d28c">
 

**멀티-헤드 어텐션은 모델이 서로 다른 위치에서 다른 표현 서브스페이스로부터 정보를 동시에 참조할 수 있게 해준다. 단일 어텐션 헤드에서는 평균화가 이를 방해한다.** (우리는 그렇지 않아서 굿굿이다는 뜻임.) 식은 아래와 같다. 
<img width="266" alt="image" src="https://github.com/user-attachments/assets/b6da1403-5014-4270-84ac-b0374419188c">

참고로 투영은 파라미터 행렬로 다음과 같이 표현된다.    
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">

이 연구에서는 h = 8개의 병렬 어텐션 레이어, 즉 헤드를 사용한다.    
즉, 각 헤드에 대해 dk = dv = d_model/h = 64를 사용한다.   
각 헤드의 차원이 줄어들었기 때문에 총 계산 비용은 전체 차원의 단일 헤드 어텐션과 유사하다.   

--- 
* 정리하자면 이런 구조다.   
  - 선형 투영: 쿼리, 키, 값을 각각 서로 다른 가중치 행렬로 선형 투영하여 차원을 줄임.   
  - 멀티-헤드: 여러 번의 투영을 병렬로 진행하여 다양한 서브스페이스의 정보를 얻음.   
  - 결합: 병렬로 계산된 출력들을 연결하고, 최종적으로 다시 선형 변화나 행렬 W_o로 투영하여 출력 값을 얻음. 이때 차원은 d_model로 다시 복원되었다. 

* 그래서 최종적으로 이걸 왜 하냐?
 단일 어텐션 헤드와 다르게 멀티 헤드 어텐션은 여러 위치에서 다른 정보를 동시에 참조할 수 있다. 즉, 동시에 병렬로 처리할 수 있게 되는 것이다!! 그러니까 RNN보다 훨씬 빠르게 학습이 가능하고 더 복자한 패턴을 효과적으로 학습할 수 있다.
--- 
### 3.2.3 Applications of Attention in our Model

Transformer는 세 가지 방식으로 multi-head attention을 사용한다.   

1. 인코더-디코더 어텐션 레이어(Encoder-Decoder Attention layer) 에서   
쿼리(Query)가 이전 디코더 레이어에서 얻고.   
메모리 키(Key)와 값(Value)은 인코더의 output에서 얻는다.   
(그래서 decoder의 각 position이 input sequence의 모든 position를 참조할 수 잇다.)   
이는 sequence-to-sequence 모델에서 일반적으로 사용하는 인코더-디코더 어텐션 메커니즘과 같다.

2. 인코더의 self-attention 레이어에서
모든 키, 값, 쿼가 같은 위치에서 온다.
인코더의 이전 레이어 output에서 키, 값, 쿼리를 얻은 것이다.
인코더의 각 position은 이전 layer의 모든 position에 참조할 ㅏ수 있다.

3. 디코더의 self-attention 레이어에서
디코더의 각 위치가 디코더 내의 모든 위치, 그 중에서도 현재 위치까지에 대해 참조할 수 있게 한다.
디코더에서는 auto-regressive 특성을 유지하기 위해 왼쪽(미래) 방향으로의 정보 흐름을 막아야 한다. (마스킹으로 미래 시점의 단어를 막는다는 의미) 이를 위해, scaled dot-product attention에서 소프트맥스의 입력 중 잘못된 연결(미래와의 연결)에 해당하는 모든 값을 -∞로 설정하여 마스킹을 적용한다. 

##  3.3 Position-wise Feed-Forward Networks
attention sub-layer 외에도, encoder와 decoder의 각 layer에는 완전 연결된 FFN(fully connected feed-forward network)을 포함하고 있으며, 이는 각 position에 대해 독립적으로 동일하게 적용된다. 이 네트워크는 두 개의 선형 변환과 그 사이에 ReLU 활성화 함수를 갖고 있다.

> FFN(x) = max(0, xW1 + b1)W2 + b2 (2)

각 position에서의 선형 변환은 동일하지만, 레이어 간에는 서로 다른 parameters를 사용합니다. 이건 커널 크기가 1인 두 개의 컨볼루션(convolution)이라고도 볼 수 있다. 입력과 출력의 차원은 d_model = 512이며, 내부 레이어의 차원은 d_ff = 2048이다.

##  3.4 Embeddings and Softmax
토큰과 출력 토큰을 차원 d_model의 벡터로 변환한다. 우리는 또한 디코더 출력을 예측된 다음 토큰 확률로 변환하기 위해 일반적으로 학습된 선형 변환과 소프트맥스 함수를 사용한다. 우리 모델에서는 두 개의 임베딩 레이어와 프리 소프트맥스 선형 변환 간에 동일한 가중치 행렬을 공유한다. 임베딩 레이어에서는 이러한 가중치를 √d_model로 곱합니다. (위에서 한 번씩 다 정리 된 내용)

<img width="370" alt="image" src="https://github.com/user-attachments/assets/0860687a-9fad-4d38-b7cc-a2a80875eb7c">

표 1: 서로 다른 레이어 유형에 대한 최대 경로 길이, 레이어별 복잡도 및 최소 순차 연산 수. n은 시퀀스 길이, d는 표현 차원, k는 컨볼루션의 커널 크기, r은 제한된 자기 주의(restricted self-attention)에서 이웃의 크기

##  3.5 Positional Encoding
트랜스포머는 순환 구조나 컨볼루션이 없기 때문에, (그래서 시퀀스의 위치 정보가 없음), 모델이 시퀀스의 순서를 활용할 수 있도록 토큰의 상대적 또는 절대적인 위치에 대한 정보를 주입한다.  

이를 위해 인코더와 디코더 스택의 하단에 입력 임베딩에 위치 인코딩(positional encodings)을 추가한다. 위치 인코딩은 임베딩과 동일한 차원 d_model을 가지므로 두 값을 더할 수 있다. 위치 인코딩에는 학습된 것과 고정된 것 등 다양한 선택이 있다.

서로 다른 주파수의 sin, cos 함수를 사용한다:

PE(pos, 2i) = sin(pos / 10000^(2i / d_model))
PE(pos, 2i + 1) = cos(pos / 10000^(2i / d_model))

pos = 위치, i = 차원이다. 즉, 위치 인코딩의 각 차원은 사인 함수에 대응한다. 파장(geometric progression wavelengths)은 2π에서 10000 · 2π까지 기하급수적으로 증가한다.   

이 함수를 선택한 이유는 고정된 오프셋 k에 대해 PE_pos+k를 PE_pos의 선형 함수로 나타낼 수 있기 때문이다. 선형 함수로 나타내면, 모델이 상대적 위치에 따라 주의를 기울이는 것을 쉽게 배울 수 있을 것이라고 가정했다. 

대신 학습된 위치 임베딩을 사용하는 실험을 했는데, 두 가지 버전이 거의 동일한 결과를 생성한다는 것을 발견했다. 사인파 버전을 선택했는데, 이는 모델이 훈련 중 접한 시퀀스 길이보다 긴 시퀀스 길이에도 추론 할 수 있기 때문이다. 


# <a id="section4"></a>4.  WhySelf-Attention
self-attention 레이어의 다양한 측면을  recurrent and convolution layers 레이어와 비교합니다.
이때 recurrent and convolution layers는 일반적으로 변수 길이의 기호 표현 시퀀스(symbol representations) (x1,..., xn)를 다른 동일 길이의 시퀀스 (z1,...,zn)로 매핑하는 데 사용되는 레이어임. 여기서 xi, zi는 Rd에 속하는 값이다. 이는 일반적인 시퀀스 변환 인코더 또는 디코더의 숨겨진(hidden) 레이어와 같은 것이다. 

self-attention 를 제안하는 이유는 3가지다.    

1. 레이어당 총 계산 복잡성   

2. 최소한의 순차 연산 수로 측정되는 병렬화 가능한 계산의 양    
  
3. 네트워크 내에서 장기 의존성(long-range dependencies) 사이의 경로 길이.
장기 의존성을 학습하는 것은 많은 시퀀스 변환 작업에서 주요 도전 과제다. 이러한 의존성을 학습하는 능력에 영향을 미치는 주요 요소 중 하나는 **네트워크에서 신호가 앞뒤로 이동해야 하는 경로의 길이**다. 입력 및 출력 시퀀스의 임의의 위치 조합 간의 경로가 짧을수록 장기 의존성을 학습하기가 더 쉬워진다. 따라서 다양한 레이어 유형으로 구성된 네트워크에서 입력 및 출력의 두 위치 간 최대 경로 길이도 비교했다.

비교한 결과를 표에서 확인할 수 있다: 
<img width="370" alt="image" src="https://github.com/user-attachments/assets/20101274-aad5-40fd-8afa-db1bd1ba786c">
* 시퀀스 길이 n, 표현 차원 d 

실험결과를 통해 제안하는 이유의 근거를 아라보자.     

1. 계산 복잡성 측면에서   
self-attention 레이어는 n < d일때 (대개 기계 번역의 최첨단 모델에서 사용되는 문장 표현의 경우에) 더 빠르다. 그리고 매우 긴 시퀀스가 포함된 작업은 계산 성능을 개선하기 위해, self-attention을 해당 출력 위치를 중심으로 입력 시퀀스 내의 크기 r인 이웃만 고려하도록 제한할 수 있다. 이렇게 하면 최대 경로 길이가 O(n/r)로 증가한다. 
   
3. 병렬화.   
self-attention 레이어는 모든 위치를 일정한 수의 순차적으로 실행된 연산으로 연결한다.   
그러나 순환 레이어는 O(n) 순차 연산을 필요로 한다.    

3. 길이
커널 너비 k < n인 단일 컨볼루션 레이어는 모든 입력 및 출력 위치 쌍을 연결하지 않는다. 이를 위해서는 연속 커널의 경우 O(n/k) 개의 컨볼루션 레이어가 필요하거나, 팽창 컨볼루션의 경우 O(logk(n))가 필요하여 네트워크 내의 임의의 두 위치 간의 가장 긴 경로의 길이가 증가한다. 컨볼루션 레이어는 일반적으로 순환 레이어보다 k배 더 비싸다. 그러나 분리 가능한 컨볼루션(separable convolutions)은 복잡성을 상당히 줄여 O(k · n · d + n · d²)로 만든다. 그러나 k = n인 경우에도 분리 가능한 컨볼루션의 복잡성은 self-attention 레이어와 포인트-와이즈(feed-forward) 레이어의 조합과 동일하며, 이는 우리가 모델에서 채택한 접근 방식이다. 

추가로, 
self-attention은 더 해석 가능한 모델을 제공할 수 있다. 
우리는 모델의 주의(attention) 분포를 검사하고 부록에서 예제를 제시하고 논의한다. 개별 attention 헤드가 분명히 서로 다른 작업을 수행하도록 학습할 뿐만 아니라, 많은 헤드가 문장의 구문 및 의미 구조와 관련된 행동을 나타내는 것처럼 보인다.


//////// 나중에...^_^
# <a id="section5"></a>5.  Training
## 5.1 Training Data and Batching
## 5.2 Hardware and Schedule
## 5.3 Optimizer
## 5.4 Regularization

# <a id="section6"></a>6.  Results
##  6.1 MachineTranslation
##  6.2 ModelVariations
##  6.3 EnglishConstituencyParsing

# <a id="section7"></a>7.  Conclusion

# <a id="section8"></a>8.  정리 
