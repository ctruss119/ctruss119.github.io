---
title:  "10월/ 1. 트랜스포머 공부하기(1) - Attention Is All You Need 논문 읽기(2)  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">3.2 ModelArchitecture/Attention </a></li>
  <li><a href="#section2">3.3 ModelArchitecture/Position-wise Feed-Forward Networks </a></li>
  <li><a href="#section3">3.4 ModelArchitecture/Embeddings and Softmax </a></li>
  <li><a href="#section4">3.5 ModelArchitecture/Positional Encoding </a></li>
  <li><a href="#section5">4.  WhySelf-Attention </a></li>
  <li><a href="#section6">5.  Training </a></li>
  <li><a href="#section7">6.  Results </a></li>
  <li><a href="#section8">7.  Conclusion </a></li>
  <li><a href="#section9">8.  정리 </a></li>  
</ul>

-------------------------------------------------------------------   
생각보다 길어져서 한 번 자르고 가는 걸로... 
---
# <a id="section1"></a>3.2 ModelArchitecture/Attention

##  3.2 Attention
어텐션 함수는 쿼리(Query)와 일련의 키-값(Key-Value)의 집합 쌍을 출력으로 매핑하는 과정으로 설명할 수 있다. 
이때 쿼리, 키, 값, 출력(query, keys, values, output)은 모두 벡터로 표현된다. 
출력은 각 값에 가중치를 곱한 값들의 가중 합(weighted sum)으로 계산된다. 이때 각 값에 할당된 가중치(weight)는 해당 값의 키와 쿼리 간의 호환성을 평가하는 함수(compatibility function)에 의해 계산된다.
* Q = 영향을 받는 벡터
* K = 영향을 주는 벡터
* V = 주는 영향의 가중치 벡터 <img width="173" alt="image" src="https://github.com/user-attachments/assets/2aab587c-aef3-4ec8-953b-decf900b62ab">


### 3.2.1 Scaled Dot-Product Attention
### 3.2.2 Multi-Head Attention
### 3.2.3 Applications of Attention in our Model

##  3.3 Position-wise Feed-Forward Networks
##  3.4 Embeddings and Softmax
##  3.5 Positional Encoding

# <a id="section4"></a>4.  WhySelf-Attention

# <a id="section5"></a>5.  Training
## 5.1 Training Data and Batching
## 5.2 Hardware and Schedule
## 5.3 Optimizer
## 5.4 Regularization

# <a id="section6"></a>6.  Results
##  6.1 MachineTranslation
##  6.2 ModelVariations
##  6.3 EnglishConstituencyParsing

# <a id="section7"></a>7.  Conclusion

# <a id="section8"></a>8.  정리 
