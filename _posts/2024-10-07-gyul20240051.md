---
title:  "10월/ 1. 트랜스포머 공부하기(1) - Attention Is All You Need 논문 읽기(2)  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">3.2 ModelArchitecture/Attention </a></li>
  <li><a href="#section2">3.3 ModelArchitecture/Position-wise Feed-Forward Networks </a></li>
  <li><a href="#section3">3.4 ModelArchitecture/Embeddings and Softmax </a></li>
  <li><a href="#section4">3.5 ModelArchitecture/Positional Encoding </a></li>
  <li><a href="#section5">4.  WhySelf-Attention </a></li>
  <li><a href="#section6">5.  Training </a></li>
  <li><a href="#section7">6.  Results </a></li>
  <li><a href="#section8">7.  Conclusion </a></li>
  <li><a href="#section9">8.  정리 </a></li>  
</ul>

-------------------------------------------------------------------   
생각보다 길어져서 한 번 자르고 가는 걸로... 
---
# <a id="section1"></a>3.2 ModelArchitecture/Attention

##  3.2 Attention
어텐션 함수는 쿼리(Query)와 일련의 키-값(Key-Value)의 집합 쌍을 출력으로 매핑하는 과정으로 설명할 수 있다. 
이때 쿼리, 키, 값, 출력(query, keys, values, output)은 모두 벡터로 표현된다. 
출력은 각 값에 가중치를 곱한 값들의 가중 합(weighted sum)으로 계산된다. 이때 각 값에 할당된 가중치(weight)는 해당 값의 키와 쿼리 간의 호환성을 평가하는 함수(compatibility function)에 의해 계산된다.
* Q = 영향을 받는 벡터 // 무엇을 찾고자 하는지를 나타내고 
* K = 영향을 주는 벡터 // 각각의 위치가 가지는 특징을 나타내고 
* V = 주는 영향의 가중치 벡터 // 각 위치에 담긴 정보를 의미함 
<img width="173" alt="image" src="https://github.com/user-attachments/assets/2aab587c-aef3-4ec8-953b-decf900b62ab">   
<img width="344" alt="image" src="https://github.com/user-attachments/assets/a164a027-aa96-4a3a-98b4-fd9684be10f2">   

### 3.2.1 Scaled Dot-Product Attention
입력은 쿼리(Query)와 키(Key)가 차원 d_k를 갖고, 값(Value)이 차원 d_v를 갖는다. Key와 query의 내적(dot product)을 계산한다. 그 각각의 값을 √(d_k)로 나눈다. 그 후 softmax 함수를 적용하여 값에 대한 weight를 구한다.

식으로 보자면 Output matrix는 다음과 같이 계산된다.: 
<img width="189" alt="image" src="https://github.com/user-attachments/assets/b1c3a1f9-8812-4a82-86d1-22edfd8e2d34">

가장 흔히 사용되는 두 가지 어텐션 함수는 (1) 가산 어텐션(additive attention)과 (2) 곱셈 어텐션(dot-product attention)이다.   

(1) 가산 어텐션은 단일 히든 레이어를 가진 FFN(feed-forward network)을 사용해 호환성 함수(compatibility function)를 계산한다.    

(2) 곱셈 어텐션은 스케일링 인자(scaling factor: 1/√(dk))를 제외하고 동일하다.    

두 메커니즘은 이론적으로는 복잡도가 비슷하지만, 실제로는 곱셈 어텐션이 훨씬 빠르고 메모리 효율적이다. 곱셈 어텐션은 고도로 최적화된 행렬 곱셈 코드를 사용할 수 있기 때문이다.

d_k 값이 작을 때에는 두 메커니즘의 성능이 비슷하지만, d_k값이 클 때에는 곱셈 어텐션보다 가산 어텐션이 더 좋은 성능을 보인다. d_k값이 클 때, 내적(dot product)이 매우 큰 값을 가지게 되어 소프트맥스 함수의 기울기(gradient)가 매우 작아지는 영역으로 밀려나기 때문이라 추측한다. 이러한 효과를 상쇄하기 위해, 우리는 내적에 1/√(dk)로 스케일링을 적용한다.

---
이해가 너무 안가니까 한 번 정리하고 넘어가자...    

(1) Scaled Dot-Product Attention은 쿼리(Query), 키(Key), 값(Value)라는 세 가지 입력을 사용하여 특정한 방법으로 주어진 값들에 "주의(attention)"를 기울여, 그에 따라 중요한 정보를 추출해낸다.   

(2) 쿼리는 질문. 키는 뭔지. 값은 그래서 전달하고 싶은 정보가 뭔지. (그러니까 얼마나 중요한지 인 것 같음)... 쿼리와 키를 내적하면 이 벡터 사이의 유사도를 계산할 수 있음. 얘네끼리 얼마나 유사한지 알 수 있다는 거임. 이 정도에 따라 어떤 값(v)가 더 중요하게 사용될지(얼마나 더 많은 가중치를 줄 지)가 결정되는 거임. 

(3) 근데 여기서 문제는 쿼리와 키의 차원이 클 때임!!!! 차원이 커지면 둘 사이의 내적 값이 커지게 됨. 근데 이러면 softmax함수를 돌릴 때 유사도의 차이를 구부하기 어려워 짐   

(4) 이 문제를 해결하기 위해 스케일링을 통해 값의 크기를 줄여서 소프트맥스함수가 균형 있게 적용되어서 다양한 값에 적절한 가중치를 줄 수 있는 거임.    

(5) 그래서 결론적으로, 쿼리와 키 간의 내적 값을 구하고 > 스케일링하고 > 그 값을 소프트맥스 함수에 통과시킴 (함수는, 내적 값을 확률처럼 변환하는 역할을 함. 즉, 값들은 0과 1 사이의 숫자로 변환되는 거임. 스케일링 안 하면 다 0이고 1이고 몰려가지고 차이를 구분하기 어렵게 되는 거.) > 이 결과가 각 값에 부여될 가중치가 되는 거였삼. 

(6) 그래서 값(value)는 소프트맥스 함수로 계산된 가중치에 의해 weighted sum된다. 이게 무슨 의미냐면, 어떤 값이 쿼리와 가장 유사한 키를 갖고 있으면, 그 값이 최종 출력에 더 큰 영향을 미치는 거임. 

(7) 이런 어텐션 계산에는 가산 어텐션과 곱셈 어텐션이 있는데 이 논문에서는 곱셈 어텐션이 가산 어텐션보다 계산에서 효율적이고, 차원이 커지면 성능이 떨어지는데, 이걸 보완하기 위해 내적에 1/√(dk)로 스케일링했다는 것. 

---
### 3.2.2 Multi-Head Attention

 d_model 차원의 키, 값, 쿼리로 하나의 어텐션 함수( single attention)를 수행하는 대신,  우리는 쿼리, 키, 값에 대해, 서로 다른 학습된 선형 투영(linear projection)을 통해 각각 dk, dk, dv 차원으로 h번 선형 투영(linear projection)하는 것이 유익하다는 것을 알았다. 

 * 이게 대체 무슨 소리냐?    
: 쿼리, 키, 값 벡터들은 원래 모두 d_model 차원의 벡터들임. 근데 선형 투영을 함. 쿼리, 키, 값을 각각 W_Q, W_K, W_V라는 선형 변환 행렬을 통해 새로운 차원으로 변환한다는 뜻임. h번 했음. 즉, 원래 d_model 차원에 있던 쿼리, 키, 값 벡터를 각각 dk, dk, dv 차원으로 변환하는 것임. dk와 dv는 보통 d_model/(헤드의 수) 임. 아무튼, 더 작은 차원으로 변환되었음. 
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">

* 그 결과물이 뭔데? : h번 서로 다른 투영 행렬을 사용하면 쿼리, 키, 벡터가 h개의 각각 다른 차원의 벡터를 얻음 

 
 이렇게 투영된 쿼리, 키, 값의 각 버전에 대해 병렬로 어텐션 함수를 수행하여 dv 차원의 출력 값을 얻는다. 이 값들을 연결(concatenate)하고 다시 한 번 투영하여 최종 값을 얻게 됩니다. 이는 아래 그림을 참고하라.   
<img width="137" alt="image" src="https://github.com/user-attachments/assets/6d102322-75ac-4efd-bb58-19cfba01d28c">
 

**멀티-헤드 어텐션은 모델이 서로 다른 위치에서 다른 표현 서브스페이스로부터 정보를 동시에 참조할 수 있게 해준다. 단일 어텐션 헤드에서는 평균화가 이를 방해한다.** (우리는 그렇지 않아서 굿굿이다는 뜻임.) 식은 아래와 같다. 
<img width="266" alt="image" src="https://github.com/user-attachments/assets/b6da1403-5014-4270-84ac-b0374419188c">

참고로 투영은 파라미터 행렬로 다음과 같이 표현된다.    
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">

이 연구에서는 h = 8개의 병렬 어텐션 레이어, 즉 헤드를 사용한다.    
즉, 각 헤드에 대해 dk = dv = d_model/h = 64를 사용한다.   
각 헤드의 차원이 줄어들었기 때문에 총 계산 비용은 전체 차원의 단일 헤드 어텐션과 유사하다.   

* 정리하자면 이런 구조다.   
  - 선형 투영: 쿼리, 키, 값을 각각 서로 다른 가중치 행렬로 선형 투영하여 차원을 줄임.   
  - 멀티-헤드: 여러 번의 투영을 병렬로 진행하여 다양한 서브스페이스의 정보를 얻음.   
  - 결합: 병렬로 계산된 출력들을 연결하고, 최종적으로 다시 선형 변화나 행렬 W_o로 투영하여 출력 값을 얻음. 이때 차원은 d_model로 다시 복원되었다. 

* 그래서 최종적으로 이걸 왜 하냐?
 단일 어텐션 헤드와 다르게 멀티 헤드 어텐션은 여러 위치에서 다른 정보를 동시에 참조할 수 있다. 즉, 동시에 병렬로 처리할 수 있게 되는 것이다!! 그러니까 RNN보다 훨씬 빠르게 학습이 가능하고 더 복자한 패턴을 효과적으로 학습할 수 있다. 
    
### 3.2.3 Applications of Attention in our Model

##  3.3 Position-wise Feed-Forward Networks
##  3.4 Embeddings and Softmax
##  3.5 Positional Encoding

# <a id="section4"></a>4.  WhySelf-Attention

# <a id="section5"></a>5.  Training
## 5.1 Training Data and Batching
## 5.2 Hardware and Schedule
## 5.3 Optimizer
## 5.4 Regularization

# <a id="section6"></a>6.  Results
##  6.1 MachineTranslation
##  6.2 ModelVariations
##  6.3 EnglishConstituencyParsing

# <a id="section7"></a>7.  Conclusion

# <a id="section8"></a>8.  정리 
