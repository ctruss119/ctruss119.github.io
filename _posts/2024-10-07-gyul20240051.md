---
title:  "10월/ 1. 트랜스포머 공부하기(1) - Attention Is All You Need 논문 읽기(2)  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">3.2 ModelArchitecture/Attention </a></li>
  <li><a href="#section2">3.3 ModelArchitecture/Position-wise Feed-Forward Networks </a></li>
  <li><a href="#section3">3.4 ModelArchitecture/Embeddings and Softmax </a></li>
  <li><a href="#section4">3.5 ModelArchitecture/Positional Encoding </a></li>
  <li><a href="#section5">4.  WhySelf-Attention </a></li>
  <li><a href="#section6">5.  Training </a></li>
  <li><a href="#section7">6.  Results </a></li>
  <li><a href="#section8">7.  Conclusion </a></li>
  <li><a href="#section9">8.  정리 </a></li>  
</ul>

-------------------------------------------------------------------   
생각보다 길어져서 한 번 자르고 가는 걸로... 
---
# <a id="section1"></a>3.2 ModelArchitecture/Attention

##  3.2 Attention
어텐션 함수는 쿼리(Query)와 일련의 키-값(Key-Value)의 집합 쌍을 출력으로 매핑하는 과정으로 설명할 수 있다. 
이때 쿼리, 키, 값, 출력(query, keys, values, output)은 모두 벡터로 표현된다. 
출력은 각 값에 가중치를 곱한 값들의 가중 합(weighted sum)으로 계산된다. 이때 각 값에 할당된 가중치(weight)는 해당 값의 키와 쿼리 간의 호환성을 평가하는 함수(compatibility function)에 의해 계산된다.
* Q = 영향을 받는 벡터 // 무엇을 찾고자 하는지를 나타내고 
* K = 영향을 주는 벡터 // 각각의 위치가 가지는 특징을 나타내고 
* V = 주는 영향의 가중치 벡터 // 각 위치에 담긴 정보를 의미함 
<img width="173" alt="image" src="https://github.com/user-attachments/assets/2aab587c-aef3-4ec8-953b-decf900b62ab">   
<img width="344" alt="image" src="https://github.com/user-attachments/assets/a164a027-aa96-4a3a-98b4-fd9684be10f2">   

### 3.2.1 Scaled Dot-Product Attention
입력은 쿼리(Query)와 키(Key)가 차원 d_k를 갖고, 값(Value)이 차원 d_v를 갖는다. Key와 query의 내적(dot product)을 계산한다. 그 각각의 값을 √(d_k)로 나눈다. 그 후 softmax 함수를 적용하여 값에 대한 weight를 구한다.

식으로 보자면 Output matrix는 다음과 같이 계산된다.: 
<img width="189" alt="image" src="https://github.com/user-attachments/assets/b1c3a1f9-8812-4a82-86d1-22edfd8e2d34">

가장 흔히 사용되는 두 가지 어텐션 함수는 (1) 가산 어텐션(additive attention)과 (2) 곱셈 어텐션(dot-product attention)이다.   

(1) 가산 어텐션은 단일 히든 레이어를 가진 FFN(feed-forward network)을 사용해 호환성 함수(compatibility function)를 계산한다.    

(2) 곱셈 어텐션은 스케일링 인자(scaling factor: 1/√(dk))를 제외하고 동일하다.    

두 메커니즘은 이론적으로는 복잡도가 비슷하지만, 실제로는 곱셈 어텐션이 훨씬 빠르고 메모리 효율적이다. 곱셈 어텐션은 고도로 최적화된 행렬 곱셈 코드를 사용할 수 있기 때문이다.

d_k 값이 작을 때에는 두 메커니즘의 성능이 비슷하지만, d_k값이 클 때에는 곱셈 어텐션보다 가산 어텐션이 더 좋은 성능을 보인다. d_k값이 클 때, 내적(dot product)이 매우 큰 값을 가지게 되어 소프트맥스 함수의 기울기(gradient)가 매우 작아지는 영역으로 밀려나기 때문이라 추측한다. 이러한 효과를 상쇄하기 위해, 우리는 내적에 1/√(dk)로 스케일링을 적용합니다.

### 3.2.2 Multi-Head Attention

 d_model 차원의 키, 값, 쿼리로 하나의 어텐션 함수( single attention)를 수행하는 대신,  우리는 쿼리, 키, 값에 대해, 서로 다른 학습된 선형 투영(linear projection)을 통해 각각 dk, dk, dv 차원으로 h번 선형 투영(linear projection)하는 것이 유익하다는 것을 알았다. 

 * 이게 대체 무슨 소리냐?    
: 쿼리, 키, 값 벡터들은 원래 모두 d_model 차원의 벡터들임. 근데 선형 투영을 함. 쿼리, 키, 값을 각각 W_Q, W_K, W_V라는 선형 변환 행렬을 통해 새로운 차원으로 변환한다는 뜻임. h번 했음. 즉, 원래 d_model 차원에 있던 쿼리, 키, 값 벡터를 각각 dk, dk, dv 차원으로 변환하는 것임. dk와 dv는 보통 d_model/(헤드의 수) 임. 아무튼, 더 작은 차원으로 변환되었음.
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">
 
 이렇게 투영된 쿼리, 키, 값의 각 버전에 대해 병렬로 어텐션 함수를 수행하여 dv 차원의 출력 값을 얻는다. 이 값들을 연결(concatenate)하고 다시 한 번 투영하여 최종 값을 얻게 됩니다. 이는 아래 그림을 참고하라.   
<img width="137" alt="image" src="https://github.com/user-attachments/assets/6d102322-75ac-4efd-bb58-19cfba01d28c">
 

멀티-헤드 어텐션은 모델이 서로 다른 위치에서 다른 표현 서브스페이스로부터 정보를 동시에 참조할 수 있게 해줍니다. 단일 어텐션 헤드에서는 평균화가 이를 방해합니다. 투영은 파라미터 행렬로 다음과 같이 표현된다.    
<img width="228" alt="image" src="https://github.com/user-attachments/assets/351209d4-b02b-4f70-b341-07d4b9a5e399">

이 연구에서는 h = 8개의 병렬 어텐션 레이어, 즉 헤드를 사용한다.    
즉, 각 헤드에 대해 dk = dv = d_model/h = 64를 사용한다. 
각 헤드의 차원이 줄어들었기 때문에 총 계산 비용은 전체 차원의 단일 헤드 어텐션과 유사하다.

* 정리하자면 이런 구조다. 
  - 선형 투영: 쿼리, 키, 값을 각각 서로 다른 가중치 행렬로 선형 투영하여 차원을 줄임.   
  - 멀티-헤드: 여러 번의 투영을 병렬로 진행하여 다양한 서브스페이스의 정보를 얻음.   
  - 결합: 병렬로 계산된 출력들을 연결하고, 최종적으로 다시 선형 변화나 행렬 W_o로 투영하여 출력 값을 얻음.   
    
### 3.2.3 Applications of Attention in our Model

##  3.3 Position-wise Feed-Forward Networks
##  3.4 Embeddings and Softmax
##  3.5 Positional Encoding

# <a id="section4"></a>4.  WhySelf-Attention

# <a id="section5"></a>5.  Training
## 5.1 Training Data and Batching
## 5.2 Hardware and Schedule
## 5.3 Optimizer
## 5.4 Regularization

# <a id="section6"></a>6.  Results
##  6.1 MachineTranslation
##  6.2 ModelVariations
##  6.3 EnglishConstituencyParsing

# <a id="section7"></a>7.  Conclusion

# <a id="section8"></a>8.  정리 
