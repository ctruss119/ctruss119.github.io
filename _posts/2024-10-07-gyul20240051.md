---
title:  "10월/ 1. 트랜스포머 공부하기(1) - Attention Is All You Need 논문 읽기(2)  "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">3.2 ModelArchitecture/Attention </a></li>
  <li><a href="#section2">3.3 ModelArchitecture/Position-wise Feed-Forward Networks </a></li>
  <li><a href="#section3">3.4 ModelArchitecture/Embeddings and Softmax </a></li>
  <li><a href="#section4">3.5 ModelArchitecture/Positional Encoding </a></li>
  <li><a href="#section5">4.  WhySelf-Attention </a></li>
  <li><a href="#section6">5.  Training </a></li>
  <li><a href="#section7">6.  Results </a></li>
  <li><a href="#section8">7.  Conclusion </a></li>
  <li><a href="#section9">8.  정리 </a></li>  
</ul>

-------------------------------------------------------------------   
생각보다 길어져서 한 번 자르고 가는 걸로... 
---
# <a id="section1"></a>3.2 Attention

##  3.2 Attention
### 3.2.1 Scaled Dot-Product Attention
### 3.2.2 Multi-Head Attention
### 3.2.3 Applications of Attention in our Model

##  3.3 Position-wise Feed-Forward Networks
##  3.4 Embeddings and Softmax
##  3.5 Positional Encoding

# <a id="section4"></a>4.  WhySelf-Attention

# <a id="section5"></a>5.  Training
## 5.1 Training Data and Batching
## 5.2 Hardware and Schedule
## 5.3 Optimizer
## 5.4 Regularization

# <a id="section6"></a>6.  Results
##  6.1 MachineTranslation
##  6.2 ModelVariations
##  6.3 EnglishConstituencyParsing

# <a id="section7"></a>7.  Conclusion

# <a id="section8"></a>8.  정리 
