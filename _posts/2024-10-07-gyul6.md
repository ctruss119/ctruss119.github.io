---
title:  "10월/ 1. 트랜스포머에 대해서 공부하기 "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">1. 트랜스포머 </a></li>   
  <li><a href="#section2">2. 트랜스포머의 구조 </a></li>   
  <li><a href="#section3">3. 어텐션 </a></li>
  <li><a href="#section4">4. Attention Is All You Need </a></li>
</ul>
-------------------------------------------------------------------   

# <a id="section1"></a>1. 트랜스포머

## 1-1. 트랜스포머의 등장 배경
### 기존 모델의 한계
### 트랜스포머가 필요한 이유 
### 트랜스포머가 NLP에서 중요한 이유
  - 병렬 처리 능력
  - 장기 의존성 문제 해결
  - 
## 1-2. 트랜스포머란 무엇인가
### 정의 및 개요
### 순차 처리 vs 병렬 처리 

## 1-3. 트랜스포머의 유형
### 인코더-디코더형 트랜스포머
### 인코더형 트랜스포머 (BERT...) 
### 디코더형 트랜스포머 (GPT...)
### 트랜스포머 기반 모델들 (BERT, GPT, T5 등)


# <a id="section2"></a>2. 트랜스포머의 구조 

## 2-1. 인코더 구조 
### Input Embedding
#### Tokenizer
#### Embedding
### Positional Encoding
### Multi-Head Attention
#### Self-Attention
### Attention 후 처리

## 2-2. 디코더 구조 
### Masked Self-Attention
### Cross-Attention
### 디코더의 후 처리


# <a id="section3"></a>3. 어텐션 

## 3-1. 어텐션 메커니즘의 개요
### 어텐션이란 무엇인가?
### 어텐션의 필요성

## 3-2. 어텐션의 작동 원리
### Query, Key, Value의 개념
### 어텐션 가중치 계산

## 3-3. Self-Attention
### Self-Attention의 역할과 장점
### 긴 문맥 처리

## 3-4. Multi-Head Attention
### 병렬적 주의 메커니즘
### 다양한 패턴 학습

# <a id="section4"></a>4. Attention Is All You Need


