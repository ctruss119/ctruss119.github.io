---
title:  "11월/ 1. sparseinst 공부하기 "
categories:
  - study
tags:
  - 10월
  - 631호 
---

<h2>목차</h2> 
<ul>
  <li><a href="#section1">1. </a></li>
  <li><a href="#section2">2.  </a></li>
  <li><a href="#section3">3.  </a></li>
</ul>
---
https://github.com/hustvl/SparseInst

# 0. Abstract

이 논문에서는 실시간 인스턴스 세그멘테이션을 위한 프레임워크를 제안한다. 
이 프레임워크는 개념적으로 새로운, 효율적이고 완전한 컨볼루션 기반이다. 

기존 대부분의 인스턴스 세그멘테이션 메소드는 object detection에 크게 의존하며, 
바운딩 박스나 dense ceters(특정 영역 내에 밀집된 중심점)을 기반으로 마스크 예측을 수행한다. 

이와 달리, 본 논문에서는 각 객체에 대한 유용한 영역을 강조하기 위해 
소수의 인스턴스 활성화 맵-sparse set of instance activation maps을 새로운 객체 표현으로 제안한다. 
이후, 강조된 영역(highlighted region)에 따라 feature을 집계하여 세그멘테이션과과 분할을 위한 instance-level features을 얻는다. 

또한, instance activation map은 이중 매칭(bipartite matching)을 기반해 이 객체를 일대잉방식으로 예측할 수 있어 
후처리에서 비최대 억제(non-maximum suppression, NMS)를 피할 수 있다. 
이 간단하지만 효과적인 구성으로 덕분에 SparseInst는 매우 빠른 추론 속도를 제공해 
COCO 벤치마크에서 40 FPS와 37.9 AP를 달성했다.  

* NMS는 다수의 겹치는 경계 상자(bounding box) 중에서 가장 적합한 하나를 선택하고 나머지 중복된 박스들을 제거하는 과정 
NMS를 피하면 
- 속도 향상: 중복 제거 과정을 건너뛰어서 빨라짐. 실시간에서는 중요함. 
- 객체 중복 처리 개선: 오탐지를 줄이는 과정에서 중요한 정보를 잃을 수도 있기에, nms 없으면 관계 파악과 정보 활용이 쉬워짐
- 모델의 유연성: 고정된 임계값을 기준으로 작동하는 nms는, 임계값이 잘못 설정되면 중요한 객체를 제거하거나 잘못된 걸 남길 수도 있음
- 객체가 서로 가까이 있을 때 더 나은 성능: nms는 교차 영역 비율이 높으면 두 객체를 분리하는데 어려워서 많은 객체 탐지가 힘듦
______________________________________________________________

# 1. Introduction
instance segmentation은 이미지 내 각 객체에 대해 인스턴스 수준의 세그멘테이션을 생성하는 것이 목표다. 
딥 컨볼루션 신경망과 오브젝트 디텍션이 발전해 최근 연구들은 인스턴스 세그멘테이션에서 큰 진보가 있었다. 
예를들어 COCO와 같은 대규모 벤치마크에서 인상적인 성과를 달성했다.
당시의 방법들은 주로 객체 검출기를 사용하여 먼저 인스턴스를 위치 지정한 후, 
영역 기반 컨볼루션 신경망이나 동적 컨볼루션 등의 방법을 통해 분할을 수행한다.
이러한 방법들은 개념적으로 직관적이며 높은 성능을 보여준다. 

그러나 real-time으로 효율적인 인스턴스 세그멘테이션은 여전히 몇 가지 제한이 존재한다. 

첫째, 앵커나 중심을 찾아서 객체를 탐지한 뒤 세그멘테이션하는데, 이는 불필요한 예측과 높은 연산 부담을 초래한다. 
예를 들어 CondInst에서는 512×512 입력에 대해 5456개의 인스턴스를 처리해야 한다. 
또한, 각 픽셀의 수용 범위가 제한되어 있어 앵커나 중심을 통해 객체를 밀집하게 지정하면 문맥 정보가 부족하게 된다. 

둘째, 대부분 자연 객체의 규모 변화를 처리하기 위해 다중 수준의 preditction을 요구하는데, 이는 불가피하게 지연 시간을 증가시킨다. 
영역 기반 방법들은 RoI-Align을 사용하여 지역 특징을 얻으므로 엣지/임베디드 장치에 알고리즘을 배포하기 어렵다. 

셋째, 후처리 과정에서 정렬 및 NMS와 같은 마스크 처리는 특히 밀집된 예측에서 시간이 많이 소모된다. 
향상된 NMS조차도 여전히 전체 시간의 10%에 해당하는 2ms가 소요된다.

그래서 이 논문에서는 실시간 인스턴스 분할을 위한 새로운 "highlight to segment" 패러다임을 제안한다. 
객체를 표현하기 위해 바운딩 박스나 중심을 사용하는 대신, 
소수의 인스턴스 활성화 맵(instance activation maps, IAM)을 사용하여 유용한 객체 영역을 강조한다. 
이는 약지도 객체 위치 지정에서 널리 사용되는 CAM에서 영감을 받았다. 
instance activation maps은 instance-aware weighted maps인데, 강조된 영역에 따라 인스턴스 수준의 feature가 직접 집계될 수 있다. 
이후, 인식 및 분할은 인스턴스 특징을 기반으로 수행된다. 



그림 2 (중심 기반 / 영역 기반 / iam 기반)

iam의 장점: 
(1) 구별 가능한 인스턴스 픽셀을 강조하고 방해되는 픽셀을 억제함
중심/영역 기반 방법에서의 잘못된 인스턴스 특징 위치 지정 문제를 개념적으로 방지함
(2) 이미지 전체에서 인스턴스 특징을 집계하여 더 많은 문맥을 제공함
(3) RoI-Align 같은 추가 연산 없이 인스턴스 특징을 계산하는 것이 매우 간단함 

이전 연구들은 앵커나 중심과 같은 공간 우선순위를 사용하여 대상을 할당하는데, 
iam은 입력에 따라 조건화되고 각 객체에 대해 임의로 작동해서
학습을 위해 hard-crafted rule을 사용해  대상을 할당하는 것은 불가능하다. 

그래서 iam의 레이블 할당을 공식화하는걸 이중 매칭 문제(bipartite maching problem, detr에서 제안함)로 다뤘다.
여기서 각 대상은 헝가리안 알고리즘을 통해 object detection과 iam에 할당된다. 
학습 중에는 bipartite maching을 통해 iam이 개별 객체를 강조하고 불필요한 예측을 막는다. 
그래서 추론 과정에서 NMS를 피할 수 있다.

그리고, 이 패러다임을 통해 효율적인 세그멘테이션 방법인 SparseInst을 구현했다.
SparseInst는 단일 수준 prediction을 적용한다. 

아키텍쳐:  (그림 3 참고)
이미지 feature을 추출하는 백본, 
단일 수준 feature에 대한 다중 규모 represeatition을 강화하는 인코더, 
iam을 계산하고 인식 및 분할을 수행하는 디코더

SparseInst는 완전한 컨볼루션 기반 프레임워크이며 검출기와 독립적임.
(1) iam을 통한 sparse 예측, (2) 단일 수준 예측,(3) 간결한 구조, (4) NMS나 정렬 없는 간단한 후처리라는 이점이 있음.
그래서 추론 속도가 매우 빠름. 
(MS-COCO 테스트 세트에서 37.9 마스크 AP와 40.0 FPS를 달성해 대부분의 최첨단 실시간 인스턴스 세그멘테이션을 능가함.
448 입력에서는 58.5 FPS로 경쟁력 있는 정확도를 달성하여 이전 방법들보다 빠름)


______________________________________________________________

# 2. Related Work

2. 관련 연구  
객체 표현에 따라 기존의 인스턴스 분할 방법은 크게 Region-based method와 Center-based method 두 가지로 나눌 수 있다.

###  Region-based method  
Faster R-CNN과 같은 detector을 사용해 객체를 detect하고 바운딩 박스를 얻는다. 
그 후 RoI Pooling 또는 RoI-Align을 적용해 픽셀 단위 분할을 위한 지역 특징을 추출한다. 

ex.  Mask R-CNN: Faster R-CNN에 mask branch를 추가해 확장한 경우. 
객체에 대한 마스크를 예측하고, end to end 인스턴스 세그멘테이션을 위한 강력한 base line이 된다. 
Mask R-CNN에서 발생하는 저품질 세그멘테이션과 및 구린 경계 문제를 해결하기 위해 
마스크 예측을 개선하는 방식들과 객체 위치를 점진적으로 향상시켜 더 정확한 마스크 예측을 가능하게 하는 캐스케이드 구조 등이 있다. 

### Center-based method
최근에는 특히 앵커가 없는 single stage detectors를 활용하는 접근이 많다. 
이 방법은 바운딩 박스 대신 중심 픽셀을 사용하여 객체를 표현하고 중심 feature을 이용해 세그멘테이션한다.
몇몇 방법들은 객체의 외곽선을 탐색하지만, 내부가 빈 객체나 여러 개로 이루어진 객체에 대해 한계가 있다. 

### bipartite matching for object detection
이중 매칭(bipartite matching)은 end to end object detection에서 널리 연구되어 후처리에서 NMS를 대체한다. 
IoU 등을 사용해 객체 간 겹침 정도를 평가하면 객체들간의 유사도를 계산할 수 있는데, 
이를 통해 예측된 바운딩 박스와 실제 객체를 비교해서 가장 유사한 객체들을 선택한다.  


-------------------------


3. Method 

3.1 인스턴스 활성화 맵
공식화. 직관적으로, 인스턴스 활성화 맵은 객체마다 유의미한 영역을 강조하기 위해 인스턴스에 따라 가중치가 부여된 맵이다. 이 맵에서 강조된 영역의 특징은 객체를 인식하고 분리하는 데 있어 의미론적으로 풍부하며 인스턴스 인식이 가능하다. 따라서 활성화 맵에 따라 특징을 직접 집계하여 인스턴스 특징으로 사용한다. 입력 이미지 특징 
𝑋
∈
𝑅
𝐷
×
(
𝐻
×
𝑊
)
X∈R 
D×(H×W)
 가 주어졌을 때, 인스턴스 활성화 맵은 다음과 같이 공식화된다: 
𝐴
=
𝐹
𝑖
𝑎
𝑚
(
𝑋
)
∈
𝑅
𝑁
×
(
𝐻
×
𝑊
)
A=F 
iam
​
 (X)∈R 
N×(H×W)
 , 여기서 
𝐴
A는 
𝑁
N개의 인스턴스 활성화 맵으로 구성된 희소 집합이고, 
𝐹
𝑖
𝑎
𝑚
(
)
F 
iam
​
 ()는 시그모이드 비선형성을 가진 간단한 네트워크이다. 이후, 인스턴스 활성화 맵을 사용하여 입력 특징 맵 
𝑋
X에서 특정 정보를 추출함으로써 희소 인스턴스 특징 집합을 얻는다: 
𝑧
=
𝐴
𝑋
𝑇
∈
𝑅
𝑁
×
𝐷
z=AX 
T
 ∈R 
N×D
 , 여기서 
𝑧
z는 이미지 내의 잠재적 객체들에 대한 특징 표현이다. 희소 인스턴스 인식 특징 
𝑧
𝑖
𝑁
z 
i
N
​
 는 후속 인식 및 인스턴스 수준의 분할에 곧바로 사용된다.

인스턴스 활성화 학습. 인스턴스 활성화 맵은 객체를 강조하기 위한 학습에서 인스턴스 마스크와 같은 명시적인 감독을 사용하지 않는다. 본질적으로 인식 및 분할을 위한 후속 모듈이 인스턴스 활성화 맵에 간접적인 감독을 제공하며, 이를 통해 
𝐹
𝑖
𝑎
𝑚
F 
iam
​
 이 유의미한 영역을 발견하도록 유도한다. 또한 이 감독은 이중 매칭 덕분에 인스턴스 인식이 가능하며, 
𝐹
𝑖
𝑎
𝑚
F 
iam
​
 이 객체를 구별하고 각 맵마다 하나의 객체만을 활성화하도록 강제한다. 그 결과, 제안된 인스턴스 활성화 맵은 개별 객체에 대해 차별적인 영역을 강조할 수 있다.

3.2 SparseInst
SparseInst는 간단하고 컴팩트하며 통합된 프레임워크로서 백본 네트워크, 인스턴스 컨텍스트 인코더, 그리고 IAM 기반 디코더로 구성된다. 백본 네트워크는 ResNet과 같이 주어진 이미지에서 다중 스케일 특징을 추출한다. 인스턴스 컨텍스트 인코더는 백본에 연결되어 더 많은 문맥 정보를 강화하고 다중 스케일 특징을 융합한다. 더 빠른 추론을 위해, 인코더는 입력 이미지에 대해 1/8 해상도의 단일 레벨 특징을 출력하며, 이 특징들은 후속 IAM 기반 디코더로 전달되어 전경 객체를 분류 및 분할을 위해 강조하는 인스턴스 활성화 맵을 생성한다.

3.3 인스턴스 컨텍스트 인코더
자연 장면의 객체들은 다양한 크기를 가지기 때문에, 이는 검출기의 성능 저하를 초래할 수 있다. 대부분의 접근법은 다양한 스케일의 객체 인식을 돕기 위해 다중 스케일 특징 융합을 사용하지만, 다중 피라미드 특징 사용은 특히 무거운 헤드를 사용하는 검출기에서 계산 부담을 증가시키고 중복 예측을 다량으로 생성하게 된다. 반면, 우리 방법은 단일 레벨 예측을 통해 빠른 추론을 목표로 한다. 다양한 스케일의 객체들에 대한 단일 레벨 특징의 한계를 고려하여, 특징 피라미드 네트워크를 재구성하고 인스턴스 컨텍스트 인코더를 제안한다. 이 인코더는 C5 이후에 피라미드 풀링 모듈을 사용하여 수용 범위를 확장하고 P3부터 P5까지의 특징을 융합하여 단일 레벨 특징의 다중 스케일 표현을 더욱 강화한다.

3.4 IAM 기반 분할 디코더
IAM 기반 분할 디코더는 인스턴스 브랜치와 마스크 브랜치를 포함하며, 두 브랜치는 256 채널의 3x3 컨볼루션 스택으로 구성된다. 인스턴스 브랜치는 인스턴스 활성화 맵 및 인식과 인스턴스 인식 커널을 위한 N개의 인스턴스 특징을 생성한다. 마스크 브랜치는 인스턴스 인식 마스크 특징을 인코딩하기 위해 설계되었다.

위치-민감 특징. 경험적으로, 객체들은 다양한 위치에 있으며 공간 위치는 인스턴스를 구별하는 단서로 사용할 수 있다. 따라서 CoordConv와 유사하게, 공간 위치의 정규화된 절대 좌표(x, y)로 구성된 2채널 좌표 특징을 구성한다. 이후 인코더 출력 특징과 좌표 특징을 연결하여 인스턴스 인식 표현을 강화한다.

인스턴스 활성화 맵 
𝐹
𝑖
𝑎
𝑚
F 
iam
​
 . 우리는 단일 활성화 맵으로 각 인스턴스를 강조하는 간단하면서도 효과적인 3x3 컨볼루션과 시그모이드를 가진 기본 
𝐹
𝑖
𝑎
𝑚
F 
iam
​
 을 채택한다. 이에 따라 인스턴스 특징 
𝑧
𝑖
z 
i
​
 는 각 잠재적 객체가 256 차원 벡터로 인코딩된 활성화 맵을 통해 얻어진다. 이후 세 개의 선형 계층을 분류, 객체성 점수, 및 마스크 커널 
𝑤
𝑖
𝑁
w 
i
N
​
 에 대해 적용한다. 보다 세밀한 인스턴스 특징을 얻기 위해, 각 객체에 대해 여러 활성화 맵을 가지는 그룹 인스턴스 활성화 맵(Group-IAM)을 제시하고 그룹에서 특징을 연결하여 인스턴스 특징을 집계한다.

IoU 인식 객체성. 일대일 할당은 대부분의 예측이 배경으로 할당되도록 하여 분류 신뢰도가 낮아지고 분류 점수와 분할 마스크 사이의 불일치를 야기할 수 있다. 이를 완화하기 위해, 우리는 IoU 인식 객체성을 도입하여 분류 출력을 조정한다. 예측된 마스크와 실제 마스크 간의 IoU를 전경 객체에 대한 목표로 채택하며, 이는 네트워크가 인스턴스를 분리할 수 있도록 돕는다. 우리는 IoU를 객체성 목표로만 사용하고, 추론 단계에서 IoU 인식 객체성 
𝑠
𝑖
s 
i
​
 를 사용해 분류 확률 
𝑝
𝑖
p 
i
​
 를 재평가한다. 최종 확률은 
𝑝
𝑖
=
𝑝
𝑖
𝑠
𝑖
p 
i
​
 =p 
i
​
 s 
i
​
 로 계산된다.

마스크 헤드. 인스턴스 브랜치에서 생성된 인스턴스 인식 마스크 커널 
𝑤
𝑖
𝑁
w 
i
N
​
 을 사용하여 각 인스턴스의 분할 마스크 
𝑚
𝑖
=
𝑤
𝑖
𝑀
m 
i
​
 =w 
i
​
 M를 생성할 수 있다. 최종 분할 마스크는 원본 해상도에 대해 1에 맞추어 업샘플링된다.

### 3.5 라벨 할당과 이중 매칭 손실

SparseInst는 고정 크기의 예측 집합을 출력하며, 수작업 규칙을 통해 실제 객체에 레이블을 할당하기는 어렵다. 이를 해결하기 위해 레이블 할당을 이중 매칭으로 정의한다. 먼저, 각 예측 \( i \)와 실제 객체 \( k \) 간의 페어와이즈 Dice 기반 매칭 점수 \( C(i, k) \)를 제안하며, 이는 분류 점수와 분할 마스크의 Dice 계수에 따라 결정된다.

\[
C(i, k) = \alpha \cdot p_{ic} \cdot \text{DICE}(m_i, t_k)
\]

여기서 \( \alpha \)는 분류와 분할의 영향을 균형 잡기 위한 하이퍼 파라미터로, 실험적으로 0.8로 설정되었다. \( c_k \)는 \( k \)-번째 실제 객체의 범주 레이블이고, \( p_{ic} \)는 예측 \( i \)의 범주 \( c_k \)에 대한 확률을 나타낸다. \( m_i \)와 \( t_k \)는 각각 예측 \( i \)와 실제 객체 \( k \)의 마스크이다. Dice 계수는 다음과 같이 정의된다:

\[
\text{DICE}(m, t) = \frac{2 \sum_{xy} m_{xy} t_{xy}}{\sum_{xy} m_{xy}^2 + \sum_{xy} t_{xy}^2}
\]

여기서 \( m_{xy} \)와 \( t_{xy} \)는 각각 예측 마스크와 실제 마스크의 픽셀 값이다. 이후 Hungarian 알고리즘을 사용하여 \( K \)개의 실제 객체와 \( N \)개의 예측 간 최적의 매칭을 찾는다.

훈련 손실은 다음과 같이 정의된다:

\[
L = \lambda_c L_{\text{cls}} + L_{\text{mask}} + \lambda_s L_s
\]

여기서 \( L_{\text{cls}} \)는 객체 분류를 위한 Focal Loss, \( L_{\text{mask}} \)는 마스크 손실, \( L_s \)는 IoU 인식 객체성을 위한 이진 교차 엔트로피 손실이다. 전체 해상도 인스턴스 분할에서 배경과 전경 간의 불균형 문제를 고려하여, Dice 손실과 픽셀 단위의 이진 교차 엔트로피 손실을 결합한 하이브리드 마스크 손실을 사용한다:

\[
L_{\text{mask}} = \lambda_{\text{dice}} L_{\text{dice}} + \lambda_{\text{pix}} L_{\text{pix}}
\]

여기서 \( L_{\text{dice}} \)와 \( L_{\text{pix}} \)는 각각 Dice 손실과 이진 교차 엔트로피 손실이며, \( \lambda_{\text{dice}} \)와 \( \lambda_{\text{pix}} \)는 각 손실의 계수이다.

### 3.6 추론

SparseInst의 추론 단계는 매우 간단하다. 주어진 이미지를 네트워크 전체를 통해 전진시키면, 분류 점수 \( p_i^N \)와 해당 원시 분할 마스크 \( m_i^N \)을 가진 \( N \)개의 인스턴스를 바로 얻을 수 있다. 그런 다음 각 인스턴스의 범주와 신뢰도 점수를 결정하고, 이진 마스크를 임계값으로 설정하여 최종 마스크를 생성한다. 정렬과 NMS가 필요하지 않아 추론 과정이 매우 빠르다.

4. 실험
이 섹션에서는 제안된 SparseInst의 정확도와 추론 속도를 MS COCO 데이터셋에서 평가하고, 프레임워크에 대한 세부적인 부가 실험과 정성적인 결과를 제공한다.

데이터셋 및 평가 지표
우리의 실험은 COCO 데이터셋([25])에서 수행된다. 이 데이터셋은 훈련용 118,000개의 이미지, 검증용 5,000개의 이미지, 테스트용 20,000개의 이미지를 포함한다. 모든 모델은 train2017에서 훈련되며, val2017에서 평가된다. 인스턴스 분할에 대해 우리는 주로 분할 마스크에 대한 AP를 보고한다. 추론 속도에 대해서는 NVIDIA RTX 2080Ti GPU에서 후처리를 포함한 초당 프레임(FPS)을 측정한다. TensorRT나 FP16을 사용하여 가속하지 않는다.

구현 세부사항
SparseInst는 Detectron2 [42]를 기반으로 구축되었으며, 8개의 GPU에서 총 64개의 이미지를 사용하는 미니 배치 크기로 훈련된다. [33]에서 제시된 훈련 스케줄을 따르며, AdamW [27] 옵티마이저를 사용하고, 초기 학습률은 5×10^-5로 설정되며, 가중치 감쇠는 0.0001로 설정된다. 모든 모델은 270,000번의 반복을 통해 훈련되며, 210,000번과 250,000번에서 학습률이 각각 10배씩 감소한다. 백본은 ImageNet으로 사전 훈련된 가중치로 초기화되며, 배치 정규화 레이어는 동결된다. 다른 모듈은 무작위로 초기화된다. 훈련 중에는 랜덤 플립과 스케일 조정이 사용된다. 이미지의 짧은 변은 416에서 640픽셀 사이로 랜덤 샘플링되며, 긴 변은 864픽셀 이하로 설정된다. 특별히 명시되지 않으면, 우리는 짧은 변 640픽셀에서 속도와 정확도를 평가한다. 손실 계수 c, dice, pix, s는 각각 2.0, 2.0, 2.0, 1.0으로 설정된다. 또한 각 이미지에 대해 N=100개의 인스턴스를 채택한다. SparseInst의 MindSpore [29] 구현도 제공된다.

4.1 주요 결과
SparseInst는 실시간 인스턴스 분할을 목표로 하고 있으므로, 우리는 정확도와 추론 속도에 대해 실시간 인스턴스 분할을 위한 최신 방법들과 SparseInst를 비교한다. 결과는 COCO test-dev에서 평가된다. 우리는 SparseInst에 그룹 인스턴스 활성화 맵과 다양한 백본을 제공하여 속도와 정확도 간의 균형을 맞춘다. ResNet-50 [16]을 채택하여 더 높은 추론 속도를 달성하고, 그 변형인 ResNet-d [17]을 사용하여 더 나은 정확도를 얻지만 더 높은 대기 시간이 발생한다. 또한 OrienMask [12]와 YOLACT [2]와 더 잘 비교하기 위해 간단한 랜덤 크롭과 더 큰 가중치 감쇠(0.05)를 적용했다. Table 1은 SparseInst가 대부분의 실시간 방법들에 비해 더 나은 성능과 더 빠른 추론 속도를 보임을 보여준다. SparseInst는 유명한 실시간 방법인 YOLACT를 현저하게 능가하며, 더 빠른 속도로 우수한 성능을 보인다. Figure 1은 속도-정확도 균형 곡선을 나타내며, 제안된 SparseInst는 R50-d와 DCN [50]을 사용하여 동료 모델들과 비교해 더 나은 균형을 이루고 있으며, 448 크기 입력에서 58.5 FPS와 35.5 mask AP를 얻는다. 이는 대부분의 실시간 방법들(30 FPS 이상)보다 우수하다.

4.2 부가 실험
우리는 SparseInst의 다양한 구성 요소에 대해 실험을 진행하였으며, 이를 통해 프레임워크에 대한 실험적 세부 사항을 조사했다.

인스턴스 컨텍스트 인코더
Table 2는 기본적인 피처 피라미드 [23]에 대한 변경이 SparseInst 성능에 미치는 영향을 보여준다. PPM(Pyramid Pooling Module)을 추가하여 더 큰 수용 영역을 만들고 더 많은 객체 컨텍스트를 제공함으로써 성능이 크게 향상된다. 특히 APL(대형 객체에 대한 AP)에서 1.5 AP와 2.2 AP가 증가한다. 또한, P3에서 P5까지의 멀티스케일 피처를 융합하면 멀티스케일 특성 표현이 향상되고 성능이 0.7 AP와 2.0 APL 만큼 개선된다. 컨텍스트 인코더는 제한된 수용 영역을 처리하고 더 나은 멀티스케일 특성을 제공하기 위해 단일 레벨 예측에서 매우 중요하며, 이는 멀티 레벨과 단일 레벨 방법 간의 차이를 좁히는 데 도움이 된다.

디코더 구조
Table 3에서는 IAM 기반 디코더에서 두 가지 브랜치의 다양한 구조를 비교한다. 기본 설정은 각 브랜치에 대해 256채널의 4개의 컨볼루션 레이어를 사용하는 것이다. 깊이나 너비를 줄이면 성능은 떨어지지만 추론 속도는 빨라진다. 채널 수를 128로 줄이면 성능이 더 낮아진다. 깊이를 4에서 6으로 늘리면 0.4 AP 향상이 있다. 속도와 정확도 간의 균형을 고려하여, 우리는 모든 실험에서 너비=256, 깊이=4를 사용한다. 좌표 피처를 추가하면 0.5 AP 향상이 있으며, 이는 시간 소비는 미미하다. Table 3은 또한 두 브랜치의 마지막 컨볼루션을 변형 가능한 컨볼루션으로 교체한 효과를 보여준다. 변형 가능한 컨볼루션[50]은 큰 객체에 대한 수용 영역을 확장시켜 성능을 향상시키지만 시간 소비가 증가(+1.7ms)한다.

인스턴스 활성화 맵
Fiam은 객체 영역을 강조하는 핵심 구성 요소이며, Table 4에서는 Fiam에 대한 다양한 설계를 평가한다. Softmax나 1x1 컨볼루션을 사용할 때 각각 0.4 AP와 1.2 AP가 감소하고, 3x3 컨볼루션을 추가해도 성능 향상이 없다. 그러나 Group-IAM을 사용하면 4개의 그룹으로 성능이 0.7 AP 향상된다.

하이브리드 마스크 손실
Table 5는 하이브리드 마스크 손실의 효과를 분석한다. 특히 Dice 손실은 마스크 예측에 필수적인 요소이며, Dice 손실을 제거하면 성능이 급격히 떨어져 AP가 8.1 포인트 감소한다. RoI 기반 방법[15]과 비교했을 때, 전체 해상도 인스턴스 분할은 배경과 전경 간의 심각한 불균형 문제를 겪는다. Dice 손실은 전경/배경 불균형에 강력하게 대응할 수 있어 전체 해상도 분할을 처리하는 데 효과적이다. Table 5에서 픽셀별 분류 손실을 추가하면 성능이 1.0 AP 향상되고, 특히 큰 객체에서는 1.8 APL 향상이 있다. 또한, 픽셀별 손실의 가중치를 증가시키면 추가적인 개선이 있다.

IoU-aware 객체 예측
Table 6에서는 제안된 IoU-aware 객체 예측 방법의 효과를 조사한 실험을 보여준다. 객체 예측을 추가하면 인스턴스 인식 피처가 향상되며 성능이 향상된다. 리스코어링 없이도 성능이 개선된다. Cross-Entropy 손실이 L1 손실보다 더 좋은 결과를 제공한다.

크로스 어텐션과의 비교
제안된 IAM은 쿼리 기반 방법들[4, 9, 38, 47]과 유사한 점이 있으며, 쿼리와 이미지 피처 간의 크로스 어텐션은 A=QX로 요약될 수 있다. IAM은 3x3 컨볼루션을 사용하여 객체 영역을 강조하는 직접적인 공간 객체 표현을 제공하며, 4-head 크로스 어텐션을 사용하면 성능이 떨어지는 경향이 있음을 Table 7에서 확인할 수 있다.

추론 시간
제안된 방법의 효율성을 이해하기 위해 각 모듈(백본, 인코더, 디코더)의 시간 소비를 분석한 Table 8에서, 백본이 전체 시간의 50% 이상을 차지하며, 전체 지연 시간은 상대적으로 작은 차이를 보인다.


4.3 타이밍
우리 프레임워크는 단일 레벨 예측, 희소한 인스턴스 집합 강조, 완전 컨볼루션 디자인, 매우 간단한 후처리(정렬이나 NMS 없이)를 사용하여 계산 비용을 절감하기 때문에 빠른 추론 속도를 자랑한다. 제안된 방법의 효율성을 더 잘 이해하기 위해 각 모듈(즉, 백본, 인코더, 디코더, 후처리)의 추론 지연 시간을 측정했다. 정확한 기록을 위해 GPU에서 비동기 실행을 비활성화했으며, 이는 전체 추론 속도를 늦춘다. 표 8은 SparseInst의 각 모듈에 대한 추론 지연 시간(ms)을 다양한 입력 해상도에서 보여준다. 특히 백본(즉, ResNet-50)이 추론 시간의 대부분을 소모하고 후처리에는 최종 분할 및 인식 결과를 처리하는 데 거의 2ms가 필요하다는 점을 주목할 만하다. 디코더의 3x3 컨볼루션은 많은 시간을 소모하며 더 효율적인 추론을 위해서는 이를 제거할 수 있다.

4.4 크로스 어텐션과의 비교
제안된 IAM은 쿼리 기반 방법들과 일부 연결이 있다. 객체 쿼리 Q와 이미지 특징 X 사이의 크로스 어텐션은 다음과 같이 간단히 공식화할 수 있다: A = QX, O = Softmax(A)XT, 여기서 A와 O는 어텐션 맵과 출력 쿼리다. 크로스 어텐션은 특히 1x1 컨볼루션에서 IAM과 유사한 형태를 가지며, 이를 1헤드 크로스 어텐션으로 볼 수 있다. 다르게 말하자면, 우리는 3x3 컨볼루션을 Fiam으로 사용하여 객체 영역을 강조하고, 이는 인스턴스 인식을 위한 직접적인 공간 객체 표현으로 작용한다. 쿼리나 1x1 컨볼루션과 비교했을 때, 3x3 컨볼루션은 더 넓은 컨텍스트와 지역 패턴을 인식할 수 있다. 또한, 우리는 IAM을 4헤드 크로스 어텐션과 100개의 쿼리를 사용하여 인스턴스 특징을 생성하는 방식으로 대체해 보았으며, 표 7은 4헤드 크로스 어텐션이 IAM과 Group-IAM에 비해 각각 0.2AP, 0.9AP 떨어짐을 보여준다.

4.5 시각화
인스턴스 활성화 맵
그림 4는 인스턴스 활성화 맵과 해당 분할 마스크에 대한 시각화를 제공한다. 각 인스턴스 활성화 맵은 객체의 두드러진 영역을 강조한다. 분할 마스크는 잘 로컬라이즈되고 인스턴스 활성화 맵과 잘 정렬된다. 또한 인스턴스 활성화 맵은 객체의 크기, 위치, 카테고스를 넘어 객체를 강조할 수 있다.
인스턴스 활성화 맵이 객체를 구별하는 방식에 대해 더 잘 이해하기 위해, 우리는 COCO val2017의 5,000개 이미지를 평균하여 12개의(총 100개) 인스턴스 활성화 맵을 시각화했다. 그림 6은 서로 다른 공간적 위치, 크기 및 모양을 강조하는 여러 인스턴스 활성화 맵을 보여주며, 이는 동일한 카테고리나 다른 카테고리의 인스턴스를 구별하는 데 기여한다.
정성적 결과
그림 5는 SparseInst의 정성적 결과를 보여준다. 제안된 SparseInst는 정확한 분할 마스크를 생성하고 세밀한 경계를 가진다. 군중 장면과 밀집된 장면에서도 SparseInst는 다른 인스턴스를 잘 구별할 수 있다.


Table 1 - COCO 인스턴스 분할 성능 비교
COCO test-dev에서 최신 인스턴스 분할 방법과 SparseInst의 성능과 속도를 비교한 표입니다. 모든 모델의 추론 속도는 NVIDIA RTX2080Ti에서 테스트되었으며, 별도로 표시된 모델은 공개된 논문에서의 속도를 인용합니다.

Table 2 - 인스턴스 컨텍스트 인코더에 대한 소멸 연구
기본 인코더(단일 레벨 예측에 비효율적)와 비교하여 PPM(Pyramid Pooling Module)을 사용해 수용 영역을 확장하면 전체 성능이 크게 향상되고, 특히 APL(대형 객체에 대한 AP)을 크게 개선했습니다. 멀티 스케일 피처를 추가로 결합해 정확도를 추가로 높였으며, 이로 인해 발생하는 추가 지연 시간은 미미합니다.

Table 3 - 디코더 구조에 대한 소멸 연구
좌표 피처(coord.)를 추가하면 0.5 AP가 개선되고, 마지막 컨볼루션을 변형 가능 컨볼루션(dconv.)으로 대체하면 특히 대형 객체에 대해 성능이 크게 향상되었습니다. 너비나 깊이를 줄이면 추론 속도는 향상되지만 성능이 저하되며, 깊이를 증가시키면 성능이 개선되지만 속도는 저하됩니다.

Table 4 - Fiam에 대한 소멸 연구
softmax나 1x1 컨볼루션을 사용할 때 각각 0.4 AP와 1.2 AP가 감소하고, ReLU를 사용하는 두 개의 3x3 컨볼루션을 추가해도 성능 향상이 없습니다. 그러나 4개의 그룹을 사용하는 Group-IAM은 성능을 0.7 AP 향상시켰습니다.

Table 5 - 하이브리드 마스크 손실에 대한 소멸 연구
다양한 하이브리드 마스크 손실의 영향을 평가한 표입니다. Dice 손실은 필수적인 요소로, BCE 손실을 추가하면 성능이 1.0 AP 개선되며, 특히 대형 객체에서 1.8 APL 개선이 있습니다.

Table 6 - IoU 기반 Objectness에 대한 소멸 연구
Objectness를 추가하면 인스턴스 인식 피처가 향상되며, 리스코어링 없이도 성능이 향상됩니다. Cross-Entropy 손실이 L1 손실보다 더 나은 결과를 제공합니다.

Table 7 - 크로스 어텐션과의 비교
100개의 쿼리를 사용해 객체를 분할하는 하나의 4-head 크로스 어텐션을 사용하는 경우를 평가한 표입니다. (Group-) IAM과 3x3 컨볼루션이 더 나은 성능을 제공합니다.

Table 8 - 추론 시간
SparseInst의 각 모듈의 추론 지연 시간을 보고한 표입니다. 백본이 전체 시간의 50% 이상을 차지합니다.

5. 결론
이 연구에서는 객체 표현을 위한 새로운 방법인 인스턴스 활성화 맵을 탐구했다. 이는 인스턴스 인식 가중치 맵으로, 객체의 중요한 영역을 강조하는 것을 목표로 한다. 그런 다음, 우리는 객체를 강조하기 위해 인스턴스 활성화 맵의 희소 집합을 활용하고, 인스턴스 수준의 인식과 분할을 위해 활성화 맵에 따라 인스턴스 특징을 집합하는 새로운 하이라이트 분할 패러다임을 제시한다. 이 패러다임을 바탕으로 우리는 SparseInst를 제안한다. 이는 개념적으로 새로운 효율적인 end-to-end 프레임워크로, 실시간 인스턴스 분할을 위한 매우 경쟁력 있는 정확도로 빠른 추론 속도를 달성한다. 광범위한 실험과 정성적 결과는 핵심 아이디어의 효과와 속도와 정확도 간의 균형에서 우수함을 입증했다. 마지막으로, 우리는 SparseInst가 end-to-end 실시간 인스턴스 분할을 위한 일반적인 프레임워크로서 실제 장면에 적용되어 그 효과성과 효율성을 입증할 수 있기를 희망한다.


